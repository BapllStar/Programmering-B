{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "I dette project vil jeg lave et objectorienteret ConvNet, der skal kunne genkende forskellige typer af tøj og sortere dem i kategorier.\n",
    "Jeg har valgt at lave dem objektorienteret, da jeg før har forsøgt at lave ANN'er, hvor det var baseret hovedsagligt ud fra funktioner, hvilket ledte til, at de hele blev meget uoverskueligt og repeterivt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importering af biblioteker\n",
    "Først vil jeg importere nogle biblioteker med foruddefinerede funktioner, der kan hjælpe med at gøre udviklingsprocessen nemmere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Matricer og vektorer\n",
    "np.random.seed(0) # For at kunne genskabe de samme resultater.\n",
    "\n",
    "from scipy import signal as signal # Til Fast Fourier Transformation Convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defineringen af et lag\n",
    "Først vil jeg definere en superklasse, lag, som de andre typer af lag skal nedarve fra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lag:\n",
    "    # __init__, også kendt som constructor, er kort for 'intialize', og \n",
    "    # er en metode, der kaldes, når et objekt af klassen Lag oprettes. \n",
    "    # Init har ikke noget output.\n",
    "    def __init__(self) -> None:\n",
    "        # Input- of outputtypen afhænger af, hvilekt slags lag det er. \n",
    "        # Men alle lag har in og out, så jeg definerer dem som None her, \n",
    "        # for at de er der, når jeg nedarver herfra.\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    # I frem-metoden, udregner vi outputtet ud fra inputtet. Hvad der sker \n",
    "    # i frem(), afhænger af, hvilket slags lag det er.\n",
    "    def frem(self, input):\n",
    "        pass\n",
    "\n",
    "    # I tilbage-metoden, udregner vi hvilke ændringer, der skal foretages \n",
    "    # på de trænede vægte og biaser, ud fra outputtet.\n",
    "    def tilbage(self, pa_output): \n",
    "        # 'pa_output' er 'partielle afledte af outputtet'. Her snakkes der\n",
    "        # om outputtet som blev udregnet i frem(). Dette er gives til laget,\n",
    "        # fra det næste lag i netværkert under bagudpropagering.\n",
    "        pass\n",
    "\n",
    "    def update(self, skridt_længde):\n",
    "        # Denne metode opdaterer vægtene og biaserne i laget, ud fra de \n",
    "        # ændringer, der blev udregnet i tilbage(). \n",
    "\n",
    "        # 'skridt_længde' er en hyperparameter, der bestemmer, hvor meget\n",
    "        # vi vil ændre på vægtene og biaserne. Denne værdi er fastsat af mig\n",
    "        # i dette tilfælde. Man kunne dog vælge at få den til at variere, alt\n",
    "        # efter hvor langt netværket er fra sit mål.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definering af FF_Lag\n",
    "Et FF-lag, eller et 'Fuldt forbundet' lag, er en type af lag, der ofte findes i Artificial Neural Networks. Her består et lag af neuroner, der er forbundet med synapser til alle de kommende lag. Her undlader jeg dog at bruge en aktiveringsfunktion, da dette kan gøres i et separat lag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dette er et fuldt forbundet lag, også kendt som et fully connected layer, eller dense layer.\n",
    "class FF(Lag):\n",
    "    def __init__(self, n_input : int, n_output : int) -> None:\n",
    "        # Her er n_input antallet af neuroner i det forrige lag, og n_output\n",
    "        # er antallet af neuroner i det næste lag.\n",
    "        # Dett skal jeg bruge for at vide, hvilket facon vægte og biaser skal have.\n",
    "        \n",
    "        self.w = np.random.randn(n_output, n_input).astype(np.float64)\n",
    "        # w er vægtene. De er initialiseret til at være tilfældige tal. Vægte er\n",
    "        # opbygget til at være samlet i en matrix, hvor højde er antaller af\n",
    "        # neuroner i det næste lag, og bredde er antallet af neuroner i det forrige lag.\n",
    "        # Vægte bliver ganget på inputtet i frem() metoden.\n",
    "        \n",
    "        self.b = np.random.randn(n_output, 1).astype(np.float64)\n",
    "        # b er biaserne. De er initialiseret til at være tilfældige tal. Biaser er\n",
    "        # opbygget til at være en vektor, hvor højden er antallet af neuroner\n",
    "        # i det næste lag.\n",
    "        # Biaser bliver lagt til vægtene i frem() metoden.\n",
    "\n",
    "        self.pa_w = np.zeros_like(self.w).astype(np.float64)\n",
    "        self.pa_b = np.zeros_like(self.b).astype(np.float64)\n",
    "        # Disse variabler er sat til None, da de først bliver brugt i tilbage() og update() metoden.\n",
    "\n",
    "        self.iterations = 0\n",
    "        pass\n",
    "    \n",
    "    def frem(self, input) -> np.ndarray:\n",
    "        # Normalt ville man have en aktiveringsfunktion her, men dette kan også gøres\n",
    "        # som et separat lag. Derfor er det udeladt her for at gøre det mere fleksibelt.\n",
    "        \n",
    "        vægtet = np.dot(self.w, input)\n",
    "        # Inputtet ganges med vægtene, for at udregne outputtet. Dette gøres ved at\n",
    "        # lave en matrix-transformation af inputvektoren.\n",
    "\n",
    "        forskudt = vægtet + self.b\n",
    "        # Biasvektoren lægges til det vægtede input. Dette kan forskyde \n",
    "\n",
    "        self.input = input\n",
    "        # Inputtet gemmes, så det kan bruges i tilbage() metoden.\n",
    "\n",
    "        return forskudt\n",
    "\n",
    "    def tilbage(self, pa_output) -> np.ndarray:\n",
    "        \n",
    "        self.pa_w += np.dot(pa_output, self.input.T)\n",
    "        # Hvis man differencerer frem() metoden med hensyn til vægtene, finder man at\n",
    "        # resultatet er inputtet ganget med partielle afledte af outputtet.\n",
    "        # Vi transponerer inputtet, da det er en kolonnevektor, så resultatet bliver \n",
    "        # en matrix, der passer til vægtenes facon.\n",
    "\n",
    "        self.pa_b += pa_output # * 1\n",
    "        # Da biasene kun adderes på, er den partielle afledte af biaserne lig med 1.\n",
    "\n",
    "        # Vi gemmer de partielle afledte, så de kan bruges i update() metoden. Grunden til at\n",
    "        # vi ikke opdaterer vægtene og biaserne med det samme, er at vi gerne vil køre mini-batches.\n",
    "        # Dette betyder at vi først vil køre en række input igennem, og derefter opdatere vægtene og biaserne.\n",
    "\n",
    "        pa_input = np.dot(self.w.T, pa_output)\n",
    "        # I denne metode har vi brugt pa_output fra det næste lag, til at udregne de ændringer,\n",
    "        # vi skal lave til vægtene og biaserne. Vi skal også bruge et pa_output til det forrige lag.\n",
    "        # Da inputtet ganges med vægtene, for at udregne outputtet, er den partielle afledte af inputtet\n",
    "        # lig med vægtene ganget med pa_output fra det næste lag. Vi transponerer vægtene, for at få\n",
    "        # de så matcher til outputtet.\n",
    "\n",
    "        self.iterations += 1\n",
    "\n",
    "        return pa_input\n",
    "    \n",
    "    def update(self, skridt_længde):\n",
    "        self.w -= skridt_længde * (self.pa_w/self.iterations)\n",
    "        self.b -= skridt_længde * (self.pa_b/self.iterations)\n",
    "        # Vægtene og biaserne opdateres ved at trække den partielle afledte af vægtene\n",
    "        # Da vi vil gå mod minimum, fremfor maksimum, trækker vi fra, og ganger\n",
    "        # med skridtlængden.\n",
    "\n",
    "        self.pa_w = np.zeros_like(self.w).astype(np.float64)\n",
    "        self.pa_b = np.zeros_like(self.b).astype(np.float64)\n",
    "        # Vi nulstiller de partielle afledte, så de er klar til næste Epoke.\n",
    "\n",
    "        self.iterations = 0\n",
    "        # Vi nulstiller antallet af iterationer, så vi kan tælle op til næste Epoke.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definering af Aktiveringsfunktionslag\n",
    "Normal finder man aktiveringsfunktioner inde i neronerne, men man kan også vælge at lave dem til et separat lag. Så kan man holde kompleksiteten nede. Hellere flere simple lag, end få komplicerede.\n",
    "<br>Desuden er det alstå også bare pænere at kigge på..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Et lag, der bruger en aktiveringsfunktion. Dette lag har ingen vægte eller biaser.\n",
    "class Funktion(Lag):\n",
    "    def __init__(self, funktion, afledt_funktion) -> None:\n",
    "        # Funktion er en aktiveringsfunktion, og afledt_funktion er sjovt nok den afledte funktionen.\n",
    "        \n",
    "        self.funktion = funktion\n",
    "        self.afledt_funktion = afledt_funktion\n",
    "        # Her gemmes funktionerne, så de kan bruges i frem() og tilbage() metoderne.\n",
    "    \n",
    "    def frem(self, input) -> np.ndarray:\n",
    "        \n",
    "        self.input = input\n",
    "        # Inputtet gemmes, så det kan bruges i tilbage() metoden.\n",
    "\n",
    "        ikke_lineært = self.funktion(input)\n",
    "        # Funktionen anvendes på inputtet, og outputtet returneres. Aktiveringsfunktioner bliver brugt\n",
    "        # til at give netværkets beslutningsbariere en ikke-lineæritet, da ellers ville kunne blive\n",
    "        # reduceret til en lineær transformation.\n",
    "        # Dette er vigtigt, da mange problemer ikke er lineært separable.\n",
    "\n",
    "        return ikke_lineært\n",
    "    \n",
    "    def tilbage(self, pa_output) -> np.ndarray:\n",
    "        # Da aktiveringsfunktioner ikke har nogle vægte eller biaser, er der ikke noget at opdatere.\n",
    "\n",
    "        pa_input = np.multiply(pa_output, self.afledt_funktion(self.input))\n",
    "        # Her tager vi Hadamard-produktet af pa_output og den afledte funktion af inputtet.\n",
    "\n",
    "        return pa_input\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For specifikke aktiveringsfunktioner, kan vi nedarve fra Funktion, og give dem en specifik funktion og afledt funktion.\n",
    "# Her bruger jeg sigmoid, da den giver nogle pæne værdier mellem 0 og 1. Den er meget ligesom en tanH-funktion, der giver\n",
    "# værdier mellem -1 og 1. Dog kan jeg godt lide den pæne form, som sigmoid har.\n",
    "class Sigmoid(Funktion):\n",
    "    def __init__(self):\n",
    "\n",
    "        sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "        afledt_sigmoid = lambda x: sigmoid(x) * (1 - sigmoid(x)) \n",
    "        # Her bruger jeg funktionelle lambda-udtryk, for at definere sigmoid og dens afledte funktion.\n",
    "        # Dette gør, at jeg kan gemme dem i en variabel, og give dem til super's constructor.\n",
    "        # Lamda-udtryk har det some regel også med at være lidt hurtigere end funktioner, men det\n",
    "        # betyder ikke meget her, fordi jeg ikke fokuserer på hastighed.\n",
    "\n",
    "        # I den afledte funktion, bruger jeg sigmoid(x) for at undgå at skulle udregne med for store tal.\n",
    "\n",
    "        super().__init__(sigmoid, afledt_sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udregning af netværkets tab\n",
    "Når vi skal træne det neurale netværk, har vi brug for at vide, hvor godt netværket klarer sig. Derfor skal vi bruge et udtryk, der hedder 'tab'. Dette kan man finde ved at sammenligne netværkets output med éns ønskede output.\n",
    "$$\n",
    "Tab = \n",
    "(Output - ØnsketOutput)^2\n",
    "$$\n",
    "Grunden til at man sætter det i anden, er fordi programmet går efter mindst muligt tab. Hvis denne værdi kunne gå i negativ, fordi det ønskede output var højere end outputtet, ville programmet blive ved med at bevæge outputtet længere væk fra det ønskede output, fordi det ville blive en lavere værdi. Dertil får man den bonus, at tabet bliver eksponintielt voldsommere, jo mere forkert outputtet er."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ikke brugbar til træning, da man kun skal bruge den afledte funktion til at køre bagudpropageringen.\n",
    "# Dog kan man bruge denne her til at holde øje med, om netværket lærer noget.\n",
    "def tab(output, ønsket_output) -> float:\n",
    "    if output == np.nan: print(\"output er nan\")\n",
    "    if ønsket_output == np.nan: print(\"ønsket_output er nan\")\n",
    "    # Tab er en funktion, der udregner hvor langt netværket er fra sit mål.\n",
    "    return np.sum((output - ønsket_output) ** 2)\n",
    "\n",
    "def pa_tab(output, ønsket_output) -> np.ndarray:\n",
    "    # Den partielle afledte af tab-funktionen med hensyn til outputtet.\n",
    "    return 2 * (output - ønsket_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definering af et Netværk\n",
    "Normalt ville man ikek behøve at definere en klasse til at køre alle udregningerne, men da jeg gerne vi kunne lave flere versioner af forskellige netværk til tests med mere, er det hurtigere at samle det til en klasse, så jeg kan bruge den på forskellige måder, uden at skulle genskrive al koden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Netværk():\n",
    "    def __init__(self, lag) -> None:\n",
    "        self.lag = lag\n",
    "        # Lag er en liste af lag, som netværket består af.\n",
    "\n",
    "    def frem(self, input) -> np.ndarray:\n",
    "        # Frem-metoden tager inputtet, og sender det igennem alle lagene i netværket.\n",
    "\n",
    "        for l in self.lag:\n",
    "            input = l.frem(input)\n",
    "            # Inputtet sendes igennem hvert lag, og outputtet bliver det nye input.\n",
    "        return input\n",
    "    \n",
    "    def __tilbage(self, pa_output) -> np.ndarray:\n",
    "        # Tilbage-metoden tager partielle afledte af outputtet, og sender dem igennem alle lagene i netværket.\n",
    "\n",
    "        for l in reversed(self.lag):\n",
    "            pa_output = l.tilbage(pa_output)\n",
    "            # Partielle afledte af outputtet sendes igennem hvert lag, og outputtet bliver det nye input.\n",
    "        return pa_output\n",
    "    \n",
    "    def __update(self, skridt_længde) -> None:\n",
    "        # Update-metoden opdaterer vægtene og biaserne i alle lagene i netværket.\n",
    "\n",
    "        for l in self.lag:\n",
    "            l.update(skridt_længde)\n",
    "            # Vægtene og biaserne i hvert lag opdateres.\n",
    "    \n",
    "    def __tab(self, output, ønsket_output) -> float:\n",
    "        # Tab-metoden tager outputtet og ønsket_outputtet, og udregner tabet.\n",
    "        # Dette er udelukkende, så vi kan se, hvor langt netværket er fra sit mål.\n",
    "        return tab(output, ønsket_output)\n",
    "    \n",
    "\n",
    "    # Jeg har valgt at lave de overstående metoder private, da de ikke skal bruges udenfor klassen.\n",
    "\n",
    "\n",
    "    def træn(self, inputs : np.ndarray, ønskede_outputs : np.ndarray, skridt_længde : float, epoker : int, batchstørrelse : int, tab_print_interval : int) -> None:\n",
    "        # Træn-metoden tager inputs, ønskede_outputs, skridt_længde, antallet af epoker og batchstørrelsen.\n",
    "        # Den træner netværket, ved at køre inputs igennem netværket, udregne tabet, og køre bagudpropagering.\n",
    "        # Dette gøres for et antal epoker, og med en batchstørrelse.\n",
    "        \n",
    "        # Mini-batch gradientnedstigning.\n",
    "        for _ in range(epoker): \n",
    "            # For hver epoke\n",
    "\n",
    "            for i in range(batchstørrelse): \n",
    "                # For hver batch\n",
    "\n",
    "                output = self.frem(inputs[i])\n",
    "                # Inputtet sendes igennem netværket, og outputtet returneres.\n",
    "\n",
    "                pa_output = pa_tab(output, ønskede_outputs[i])\n",
    "                # Den partielle afledte af tabet for outputtet udregnes.\n",
    "\n",
    "                self.__tilbage(pa_output)\n",
    "                # Partielle afledte sendes igennem netværket.\n",
    "            \n",
    "            self.__update(skridt_længde)\n",
    "            # Efter at have kørt en batch igennem, opdateres vægtene og biaserne.\n",
    "\n",
    "            if _ % tab_print_interval == 0:\n",
    "                print(self.__tab(output, ønskede_outputs[i]))\n",
    "                # Her printes tabet, så vi kan se, om netværket lærer noget.\n",
    "        \n",
    "        print(\"\\nTræning færdig\")\n",
    "        print(f\"Afsluttet med tab: {self.__tab(output, ønskede_outputs[i])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test af ANN\n",
    "Al den overstående kode er nok til at definere et ANN, så før jeg bevæger mig videre til at tilbygge Conv-lag på ANN'et, vil jeg lige teste om det fungerer. Til dette bruger jeg den klassiske XOR test, der er ikke-lineært separabelt problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8247279374202696\n",
      "0.2486813344952613\n",
      "0.2510344233978957\n",
      "0.24649596383124286\n",
      "0.2298455072475089\n",
      "0.193482614863555\n",
      "0.13838987454624507\n",
      "0.09095274070378688\n",
      "0.061116828953222326\n",
      "0.04352230782837874\n",
      "0.032768929922133\n",
      "0.025807042516000685\n",
      "0.02104697796010895\n",
      "0.017638222838066007\n",
      "0.01510223579313258\n",
      "0.013155410790473736\n",
      "0.011621523406481597\n",
      "0.010386432707862384\n",
      "0.009373500621181394\n",
      "0.008529640573789299\n",
      "\n",
      "Træning færdig\n",
      "Afsluttet med tab: 0.007818383845293847\n"
     ]
    }
   ],
   "source": [
    "ann = Netværk([\n",
    "    FF(2, 3), \n",
    "    Sigmoid(), \n",
    "    FF(3, 1), \n",
    "    Sigmoid()\n",
    "    ])\n",
    "# Her opretter jeg et netværk, som består af et inputlag med 2 neuroner, et skjult lag med 3 neuroner,\n",
    "# et outputlag med 1 neuron.\n",
    "\n",
    "inputs = np.reshape([[0, 0], [0, 1], [1, 0], [1, 1]], (4, 2, 1))\n",
    "ønskede_outputs = np.reshape([[0], [1], [1], [0]], (4, 1, 1))\n",
    "# Her opretter jeg inputs og ønskede_outputs, som er XOR-gate. Dette er et klassisk problem, som\n",
    "\n",
    "ann.træn(inputs, ønskede_outputs, 0.1, 10000, 4, 500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her man man se, at vi har opnået et relativt lavt tab. Nu vil jeg prøve at køre den igennem for at se dets resultater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,0] : Expected 0 : Output 0.0\n",
      "[1,0] : Expected 1 : Output 1.0\n",
      "[0,1] : Expected 1 : Output 1.0\n",
      "[1,1] : Expected 0 : Output 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"[0,0] : Expected 0 : Output {np.round(ann.frem(np.array([[0], [0]]))[0,0])}\")\n",
    "print(f\"[1,0] : Expected 1 : Output {np.round(ann.frem(np.array([[1], [0]]))[0,0])}\")\n",
    "print(f\"[0,1] : Expected 1 : Output {np.round(ann.frem(np.array([[0], [1]]))[0,0])}\")\n",
    "print(f\"[1,1] : Expected 0 : Output {np.round(ann.frem(np.array([[1], [1]]))[0,0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den svarer rigtigt på opgaven."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tid til ConvNet\n",
    "Men når man har med billedgenkendelse at gøre, er det en god idé at montere et ConvNet i starten af sit ArtNet. Så det er det jeg har tænkt mig at gøre nu. Det første jeg vil gøre, er at definere et ConvLag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(Lag):\n",
    "    def __init__(self, input_facon : tuple, kernel_størrelse: int, dybde : int) -> None:\n",
    "        \n",
    "        input_dybde, input_højde, input_bredde = input_facon\n",
    "        # Her definerer jeg forskellgie variabler til at beskrive input faconen. Detter er blot for\n",
    "        # at gøre det nemmere at læse koden.\n",
    "\n",
    "        self.dybde = dybde\n",
    "        # Antal af kernels.\n",
    "\n",
    "        self.input_facon = input_facon\n",
    "        self.input_dybde = input_dybde\n",
    "        # Billeder har ofte en dybde, som beskriver hvor mange kanaler de har. Dette kunne være RGB, som\n",
    "        # har 3 kanaler, eller sort-hvid, som kun har 1 kanal.\n",
    "\n",
    "        self.output_facon = (dybde, input_højde - kernel_størrelse + 1, input_bredde - kernel_størrelse + 1)\n",
    "        # Der er lige så mange output som der er kernels.\n",
    "\n",
    "        self.kernel_facon = (dybde, input_dybde, kernel_størrelse, kernel_størrelse)\n",
    "        self.kernels = np.random.randn(*self.kernel_facon).astype(np.float64)\n",
    "        self.biases = np.random.randn(*self.output_facon).astype(np.float64)\n",
    "        # Her definerer jeg kernels som en 4D tensor. Men det kan være til fordel at forestille\n",
    "        # sig dem som flere 3D tensorer, der har hvert sit output.\n",
    "\n",
    "        \n",
    "        self.pa_kernels = np.zeros(self.kernel_facon).astype(np.float64)\n",
    "        self.pa_biases = np.zeros(self.output_facon).astype(np.float64)\n",
    "        # Disse variabler er sat til None, da de først bliver brugt i tilbage() og update() metoden.\n",
    "        \n",
    "        self.iterations = 0\n",
    "\n",
    "    def frem(self, input) -> np.ndarray:\n",
    "        \n",
    "        self.input = input\n",
    "\n",
    "        self.output = np.copy(self.biases)\n",
    "        # Her kopierer jeg biasene, så jeg ikke behøver at addere dem senere.\n",
    "\n",
    "        for i in range(self.dybde):\n",
    "            for j in range(self.input_dybde):\n",
    "                self.output[i] += signal.correlate2d(input[j], self.kernels[i, j], mode='valid')\n",
    "        # Her bruger jeg scipy's fftconvolve funktion, som er hurtigere end numpy's convolve funktion.\n",
    "        # Den bruger Fast Fourier Transformation til at udregne konvolutionen, hvilket er hurtigere end\n",
    "        # at gøre det direkte. Eller... Det er faktisk ikke konvolution. Det er korrelation, men det er\n",
    "        # næsten det samme.\n",
    "        # Jeg bruger mode='valid', da jeg ikke vil have padding. Dette betyder at outputtet bliver mindre\n",
    "        # end inputtet.\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def tilbage(self, pa_output) -> np.ndarray:\n",
    "        \n",
    "        pa_kernels = np.zeros(self.kernel_facon).astype(np.float64)\n",
    "        pa_input = np.zeros(self.input_facon).astype(np.float64)\n",
    "\n",
    "        for i in range(self.dybde):\n",
    "            for j in range(self.input_dybde):\n",
    "                pa_kernels[i, j] = signal.correlate2d(self.input[j], pa_output[i], mode='valid')\n",
    "                # Når man skal udregne de partielle afledte af kernels, kan man forkorte det til at være\n",
    "                # inputtet korreleret med de partielle afledte af outputtet.\n",
    "\n",
    "                pa_input[j] += signal.convolve2d(pa_output[i], self.kernels[i, j], mode='full')\n",
    "                # For at udregne de partielle afledte af inputtet, kan man bruge de partielle afledte af outputtet\n",
    "                # konvolveret med kernels.\n",
    "\n",
    "        # Her udregner jeg de partielle afledte af kernels og inputtet.\n",
    "\n",
    "        self.pa_kernels = pa_kernels\n",
    "        self.pa_biases = pa_output\n",
    "        # Jeg gemmer de partielle afledte, så de kan bruges i update metoden.\n",
    "\n",
    "        return pa_input\n",
    "    \n",
    "    def update(self, skridt_længde):\n",
    "        self.kernels -= skridt_længde * (self.pa_kernels/self.iterations)\n",
    "        self.biases -= skridt_længde * (self.pa_biases/self.iterations)\n",
    "        # Vægtene og biaserne opdateres ved at trække den partielle afledte af vægtene\n",
    "        # Da vi vil gå mod minimum, fremfor maksimum, trækker vi fra, og ganger\n",
    "        # med skridtlængden.\n",
    "\n",
    "        self.pa_kernels = np.zeros(self.kernel_facon).astype(np.float64)\n",
    "        self.pa_biases = np.zeros(self.output_facon).astype(np.float64)\n",
    "        # Vi nulstiller de partielle afledte, så de er klar til næste Epoke.\n",
    "\n",
    "        self.iterations = 0\n",
    "        # Vi nulstiller antallet af iterationer, så vi kan tælle op til næste Epoke."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definering af formateringslag\n",
    "Et ConvNets lag er ofte multidimensionellen, mens et ArtNets her det med at være 1D. Derfor skal vi omformatere faconen fra ConvNettet til ArtNettet, så de kan snakke sammen. NumPy har heldigvis en funktion der kan gøre det hele for mig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Formatering(Lag):\n",
    "    def __init__(self, input_facon : tuple, output_facon : tuple) -> None:\n",
    "        self.input_facon = input_facon\n",
    "        self.output_facon = output_facon\n",
    "        pass\n",
    "\n",
    "    def frem(self, input : np.ndarray) -> np.ndarray:\n",
    "        return np.reshape(input, self.output_facon)\n",
    "    \n",
    "    def tilbage(self, pa_output : np.ndarray) -> np.ndarray:\n",
    "        return np.reshape(pa_output, self.input_facon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test af ConvNet og ArtNet\n",
    "Nu skal vi se om det hele spiller sammen. Jeg vil teste det på [dette datasæt](https://www.kaggle.com/datasets/agrigorev/clothing-dataset-full). Jeg vil vælge 10 af kategorierne derfra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Netværk([\n",
    "    Conv((1, 28, 28), 3, 5),\n",
    "    Sigmoid(),\n",
    "    Formatering((3, 24, 24), (3*24*24, 1)),\n",
    "    FF(3*24*24, 16),\n",
    "    Sigmoid(),\n",
    "    FF(16, 10),\n",
    "    Sigmoid()\n",
    "    ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
