{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making some an awesome neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [None]\n",
    "biases = [None]\n",
    "activations = []\n",
    "\n",
    "layers = [5,4,5,3]\n",
    "weighted_inputs = [None] # The layer[0] doesn't have weights or biases. Therefore not a weighted input.\n",
    "\n",
    "e = 2.7182818284590452353602874713527"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_matrix(rows, columns, max, min=0):\n",
    "    matrix = np.random.uniform(min, max, size=(rows, columns))\n",
    "    return matrix\n",
    "\n",
    "def create_random_vector(rows, max, min=0):\n",
    "    vector = np.random.uniform(min, max, size=(rows,))\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_network():\n",
    "    # We start at one, because this loop handles the previous layer as well.\n",
    "    for i in range(1,len(layers)):\n",
    "        \n",
    "        previous_layer_size = layers[i-1]\n",
    "        current_layer_size = layers[i]\n",
    "\n",
    "        # Generate the weights for layer i\n",
    "        matrix = create_random_matrix(\n",
    "            current_layer_size,\n",
    "            previous_layer_size, \n",
    "            1,\n",
    "            -1\n",
    "        )\n",
    "        weights.append(matrix)\n",
    "\n",
    "        # Generate the biases for layer i\n",
    "        vector = create_random_vector(\n",
    "            current_layer_size,\n",
    "            1,\n",
    "            -1\n",
    "        )\n",
    "        biases.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# My activation function of choice. Sigmoid returns a value between 1 and 0.\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + e**(-x))\n",
    "\n",
    "# Applies the sigmoid function to every value in the vector.\n",
    "def vector_sigmoid(vector):\n",
    "    sig_vector = np.vectorize(sigmoid)(vector)\n",
    "    return sig_vector\n",
    "\n",
    "# Derivative of the sigmoid function.\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the values of a single layer.\n",
    "def calculate_layer(layer):\n",
    "    # z(L) = b(L) + W(L) * a(L-1)\n",
    "    weighted_input = np.add(biases[layer], np.dot(weights[layer], activations[layer - 1]))\n",
    "    weighted_inputs.append(weighted_input)\n",
    "    return vector_sigmoid(weighted_input)\n",
    "\n",
    "\n",
    "# Loops through every layer and calculates their activation values.\n",
    "def calculate_output(input_activation):\n",
    "    global activations\n",
    "    activations.append(input_activation)\n",
    "\n",
    "    for i in range(1,len(layers)):\n",
    "        activations.append(calculate_layer(i))\n",
    "\n",
    "    return activations[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the cost of the network.\n",
    "def vector_cost(a, y):\n",
    "    # C = sum((a-y)^2)\n",
    "    sum = 0\n",
    "    for i in range(len(a)-1):\n",
    "        sum += (a[i]-y[i])**2\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the derivative of the chain rule.\n",
    "def calculate_chain_derivative(is_parent, layer, row, column):\n",
    "    sum = 0\n",
    "    if layer == len(activations) - 1:\n",
    "        cost_derivative = 2 * (activations[layer][row] - expected_output[row])\n",
    "        sum = cost_derivative\n",
    "    else:\n",
    "        for i in range(len(activations[layer+1])):\n",
    "            sum += calculate_weight_derivative(False, layer + 1, i, row)\n",
    "    weighted_input = weighted_inputs[layer][row]\n",
    "    activation_derivative = sigmoid_derivative(weighted_input)\n",
    "    if is_parent:\n",
    "        return sum * activation_derivative\n",
    "    else:\n",
    "        weighted_input_derivative = weights[layer][row][column]\n",
    "    return sum * activation_derivative * weighted_input_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the derivative of the bias.\n",
    "def calculate_bias_derivative(is_parent, layer, row, column): # Literally just a useless function.\n",
    "    chain_derivative = calculate_chain_derivative(is_parent, layer, row, column)\n",
    "    return chain_derivative\n",
    "\n",
    "# Calculates the derivative of the weight.\n",
    "def calculate_weight_derivative(is_parent, layer, row, column):\n",
    "    chain_derivative = calculate_chain_derivative(is_parent, layer, row, column)\n",
    "    return chain_derivative * activations[layer-1][column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate():\n",
    "    gradient = []\n",
    "\n",
    "    # calculate all the weight derivatives\n",
    "\n",
    "    # List of matrices\n",
    "    weight_derivatives = [None]\n",
    "    for layer in range(1, len(weights)):\n",
    "        # Matrix\n",
    "        matrix = np.zeros_like(weights[layer])\n",
    "        for row in range(len(weights[layer])):\n",
    "            # Vector\n",
    "            vector = np.zeros_like(weights[layer][row])\n",
    "            for column in range(len(weights[layer][row])):\n",
    "                weight_derivative = calculate_weight_derivative(\n",
    "                    True,\n",
    "                    layer,\n",
    "                    row,\n",
    "                    column\n",
    "                )\n",
    "                vector[column] = weight_derivative\n",
    "            matrix[row] = vector\n",
    "        weight_derivatives.append(matrix)\n",
    "    gradient.append(weight_derivatives)\n",
    "\n",
    "    # calculate all the bias derivatives\n",
    "\n",
    "    # List of vectors\n",
    "    bias_derivatives = [None]\n",
    "    for layer in range(1, len(biases)):\n",
    "        # Vector\n",
    "        vector = np.zeros_like(biases[layer])\n",
    "        for row in range(len(biases[layer])):\n",
    "            bias_derivative = calculate_bias_derivative(\n",
    "                True,\n",
    "                layer,\n",
    "                row,\n",
    "                column\n",
    "            )\n",
    "            vector[row] = bias_derivative\n",
    "        bias_derivatives.append(vector)\n",
    "    gradient.append(bias_derivatives)\n",
    "\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(gradient,learn_rate):\n",
    "    for layer in range(1,len(weights)):\n",
    "        np.add(weights[layer], learn_rate * gradient[0][layer])\n",
    "        np.add(biases[layer], learn_rate * gradient[1][layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gradients(gradient1, gradient2):\n",
    "    for layer in range(1,len(gradient1)):\n",
    "        np.add(gradient1[layer], gradient2[layer])\n",
    "\n",
    "def divide_gradient(gradient, divisor):\n",
    "    for layer in range(1,len(gradient)):\n",
    "        np.divide(gradient[layer], divisor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "np.add([1,2,[2,3]], [1,2,[2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(inputs, expected_outputs, learn_rate):\n",
    "    \n",
    "    for input in inputs:\n",
    "        output = calculate_output(input)\n",
    "        gradient = backpropagate()\n",
    "    \n",
    "    gradient = backpropagate()\n",
    "    update_parameters(gradient, learn_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the program\n",
    "generate_network()\n",
    "\n",
    "\n",
    "\n",
    "output_activation =  calculate_output([0.5, 1, 0.2, 0, 0.1])\n",
    "\n",
    "expected_output = [1, 0, 0.5]\n",
    "cost = vector_cost(output_activation, expected_output);\n",
    "\n",
    "gradient = backpropagate()\n",
    "\n",
    "\n",
    "update_parameters(gradient, 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
