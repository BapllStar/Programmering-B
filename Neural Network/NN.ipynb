{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making some an awesome neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [None]\n",
    "biases = [None]\n",
    "activations = []\n",
    "\n",
    "layers = [5,4,5,3]\n",
    "weighted_inputs = [None] # The layer[0] doesn't have weights or biases. Therefore not a weighted input.\n",
    "\n",
    "e = 2.7182818284590452353602874713527"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_matrix(rows, columns, max, min=0):\n",
    "    matrix = np.random.uniform(min, max, size=(rows, columns))\n",
    "    return matrix\n",
    "\n",
    "def create_random_vector(rows, max, min=0):\n",
    "    vector = np.random.uniform(min, max, size=(rows,))\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_network():\n",
    "    # We start at one, because this loop handles the previous layer as well.\n",
    "    for i in range(1,len(layers)):\n",
    "        \n",
    "        previous_layer_size = layers[i-1]\n",
    "        current_layer_size = layers[i]\n",
    "\n",
    "        # Generate the weights for layer i\n",
    "        matrix = create_random_matrix(\n",
    "            current_layer_size,\n",
    "            previous_layer_size, \n",
    "            1,\n",
    "            -1\n",
    "        )\n",
    "        weights.append(matrix)\n",
    "\n",
    "        # Generate the biases for layer i\n",
    "        vector = create_random_vector(\n",
    "            current_layer_size,\n",
    "            1,\n",
    "            -1\n",
    "        )\n",
    "        biases.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# My activation function of choice. Sigmoid returns a value between 1 and 0.\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + e**(-x))\n",
    "\n",
    "# Applies the sigmoid function to every value in the vector.\n",
    "def vector_sigmoid(vector):\n",
    "    sig_vector = np.vectorize(sigmoid)(vector)\n",
    "    return sig_vector\n",
    "\n",
    "# Derivative of the sigmoid function.\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the values of a single layer.\n",
    "def calculate_layer(layer):\n",
    "    # z(L) = b(L) + W(L) * a(L-1)\n",
    "    weighted_input = np.add(biases[layer], np.dot(weights[layer], activations[layer - 1]))\n",
    "    weighted_inputs.append(weighted_input)\n",
    "    return vector_sigmoid(weighted_input)\n",
    "\n",
    "\n",
    "# Loops through every layer and calculates their activation values.\n",
    "def calculate_output(input_activation):\n",
    "    global activations\n",
    "    activations.append(input_activation)\n",
    "\n",
    "    for i in range(1,len(layers)):\n",
    "        activations.append(calculate_layer(i))\n",
    "\n",
    "    return activations[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the cost of the network.\n",
    "def vector_cost(a, y):\n",
    "    # C = sum((a-y)^2)\n",
    "    sum = 0\n",
    "    for i in range(len(a)-1):\n",
    "        sum += (a[i]-y[i])**2\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the derivative of the chain rule.\n",
    "def calculate_chain_derivative(is_parent, layer, row, column):\n",
    "    sum = 0\n",
    "    if layer == len(activations) - 1:\n",
    "        cost_derivative = 2 * (activations[layer][row] - expected_output[row])\n",
    "        sum = cost_derivative\n",
    "    else:\n",
    "        for i in range(len(activations[layer+1])):\n",
    "            sum += calculate_weight_derivative(False, layer + 1, i, row)\n",
    "    weighted_input = weighted_inputs[layer][row]\n",
    "    activation_derivative = sigmoid_derivative(weighted_input)\n",
    "    if is_parent:\n",
    "        return sum * activation_derivative\n",
    "    else:\n",
    "        weighted_input_derivative = weights[layer][row][column]\n",
    "    return sum * activation_derivative * weighted_input_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the derivative of the bias.\n",
    "def calculate_bias_derivative(is_parent, layer, row, column): # Literally just a useless function.\n",
    "    chain_derivative = calculate_chain_derivative(is_parent, layer, row, column)\n",
    "    return chain_derivative\n",
    "\n",
    "# Calculates the derivative of the weight.\n",
    "def calculate_weight_derivative(is_parent, layer, row, column):\n",
    "    chain_derivative = calculate_chain_derivative(is_parent, layer, row, column)\n",
    "    return chain_derivative * activations[layer-1][column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_derivatives = []\n",
    "for layer in range(1,len(weights)):\n",
    "    # Matrix\n",
    "    matrix = []\n",
    "    for row in range(len(weights[layer])):\n",
    "        # Vector\n",
    "        vector = []\n",
    "        for column in range(len(weights[layer][row])):\n",
    "            weight_derivative = calculate_weight_derivative(\n",
    "                True,\n",
    "                layer,\n",
    "                row,\n",
    "                column\n",
    "            )\n",
    "            vector.append(weight_derivative)\n",
    "        matrix.append(vector)\n",
    "    weight_derivatives.append(matrix)\n",
    "    gradient.append(weight_derivatives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate():\n",
    "    gradient = []\n",
    "\n",
    "    # calculate all the weight derivatives\n",
    "\n",
    "    # List of matrices\n",
    "    weight_derivatives = []\n",
    "    for layer in range(1, len(weights)):\n",
    "        # Matrix\n",
    "        matrix = np.zeros_like(weights[layer])\n",
    "        for row in range(len(weights[layer])):\n",
    "            # Vector\n",
    "            vector = np.zeros_like(weights[layer][row])\n",
    "            for column in range(len(weights[layer][row])):\n",
    "                weight_derivative = calculate_weight_derivative(\n",
    "                    True,\n",
    "                    layer,\n",
    "                    row,\n",
    "                    column\n",
    "                )\n",
    "                vector[column] = weight_derivative\n",
    "            matrix[row] = vector\n",
    "        weight_derivatives.append(matrix)\n",
    "    gradient.append(weight_derivatives)\n",
    "\n",
    "    # calculate all the bias derivatives\n",
    "\n",
    "    # List of vectors\n",
    "    bias_derivatives = []\n",
    "    for layer in range(1, len(biases)):\n",
    "        # Vector\n",
    "        vector = np.zeros_like(biases[layer])\n",
    "        for row in range(len(biases[layer])):\n",
    "            bias_derivative = calculate_bias_derivative(\n",
    "                True,\n",
    "                layer,\n",
    "                row,\n",
    "                column\n",
    "            )\n",
    "            vector[row] = bias_derivative\n",
    "        bias_derivatives.append(vector)\n",
    "    gradient.append(bias_derivatives)\n",
    "\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(gradient,learn_rate):\n",
    "    for layer in range(1,len(weights)):\n",
    "        np.add(weights[layer], learn_rate * gradient[0][layer-1])\n",
    "        np.add(biases[layer], learn_rate * gradient[1][layer-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, array([ 0.19112718, -0.79442128,  0.87420348,  0.48644452]), array([-0.3551773 , -0.06607465, -0.3449589 ,  0.57839001,  0.32339057]), array([-0.09459118, -0.6258225 , -0.30991598])]\n",
      "Output:\n",
      "[0.46611446 0.29759792 0.38559965]\n",
      "Cost: 0.3735982945811435\n",
      "Gradient:\n",
      "[[-0.09056774 -0.11781622 -0.12092452 -0.08380387 -0.1155792 ]\n",
      " [ 0.04240628  0.05516476  0.05662015  0.03923925  0.05411733]\n",
      " [-0.01847567 -0.02403431 -0.0246684  -0.01709585 -0.02357796]]\n"
     ]
    }
   ],
   "source": [
    "# running the program\n",
    "generate_network()\n",
    "print(biases)\n",
    "output_activation =  calculate_output([0.5, 1, 0.2, 0, 0.1])\n",
    "print(\"Output:\")\n",
    "print(output_activation)\n",
    "expected_output = [1, 0, 0.5]\n",
    "cost = vector_cost(output_activation, expected_output);\n",
    "print(\"Cost: \" + str(cost))\n",
    "gradient = backpropagate()\n",
    "print(\"Gradient:\")\n",
    "print(gradient)\n",
    "update_parameters(gradient, 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
