{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My activation function of choice. Sigmoid returns a value between 1 and 0.\n",
    "def sigmoid(x) -> float:\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Applies the sigmoid function to every value in the vector.\n",
    "def vector_sigmoid(vector) -> np.ndarray:\n",
    "    sig_vector = np.vectorize(sigmoid)(vector)\n",
    "    return sig_vector\n",
    "\n",
    "# Derivative of the sigmoid function.\n",
    "def sigmoid_derivative(x) -> float:\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the values of a single layer.\n",
    "def calculate_layer(layer : int) -> None:\n",
    "    global weights, biases, activations, weighted_inputs\n",
    "    # z(L) = b(L) + W(L) * a(L-1)\n",
    "    print(activations)\n",
    "    weighted_input = np.add(biases[layer], np.dot(weights[layer], activations[layer-1]))\n",
    "    weighted_inputs[layer] = weighted_input\n",
    "    activations[layer] = vector_sigmoid(weighted_input)\n",
    "\n",
    "\n",
    "# Loops through every layer and calculates their activation values.\n",
    "def calculate_output() -> None:\n",
    "    global layers, activations\n",
    "    for i in range(1,len(layers)):\n",
    "        activations[i] = calculate_layer(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the derivative of the chain rule.\n",
    "def calculate_chain_derivative(is_parent, layer, row, column) -> float:\n",
    "    global weights, weighted_inputs, activations, expected_output\n",
    "    sum = 0\n",
    "    if layer == len(activations) - 1:\n",
    "        cost_derivative = 2 * (activations[layer][row] - expected_output[row])\n",
    "        sum = cost_derivative\n",
    "    else:\n",
    "        for i in range(len(activations[layer+1])):\n",
    "            sum += calculate_chain_derivative(False, layer + 1, i, row)\n",
    "    weighted_input = weighted_inputs[layer][row]\n",
    "    activation_derivative = sigmoid_derivative(weighted_input)\n",
    "    if is_parent:\n",
    "        return sum * activation_derivative\n",
    "    else:\n",
    "        weighted_input_derivative = weights[layer][row][column]\n",
    "        return sum * activation_derivative * weighted_input_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the derivative of the bias.\n",
    "def calculate_bias_derivative(is_parent, layer, row, column): # Literally just a useless function.\n",
    "    chain_derivative = calculate_chain_derivative(is_parent, layer, row, column)\n",
    "    return chain_derivative\n",
    "\n",
    "# Calculates the derivative of the weight.\n",
    "def calculate_weight_derivative(is_parent, layer, row, column):\n",
    "    chain_derivative = calculate_chain_derivative(is_parent, layer, row, column)\n",
    "    return chain_derivative * activations[layer-1][column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate() -> None:\n",
    "    \n",
    "    global weights, biases, activations, weighted_inputs, expected_output, weight_gradient, bias_gradient\n",
    "\n",
    "    for layer in range(1, len(weights)):\n",
    "        for row in range(len(weights[layer])):\n",
    "            for column in range(len(weights[layer][row])):\n",
    "                weight_gradient[layer][row][column] = calculate_weight_derivative(True, layer, row, column)\n",
    "    \n",
    "    for layer in range(1, len(biases)):\n",
    "        for row in range(len(biases[layer])):\n",
    "            bias_gradient[layer][row] = calculate_bias_derivative(True, layer, row, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_input(input : np.ndarray) -> None:\n",
    "    global activations\n",
    "    activations[0] = input\n",
    "\n",
    "def set_expected_output(output : np.ndarray) -> None:\n",
    "    global expected_output\n",
    "    expected_output = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_sample():\n",
    "    global index, data\n",
    "    index += 1\n",
    "    print(index)\n",
    "    sample = data[index]\n",
    "    set_input(sample[0])\n",
    "    set_expected_output(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn ():\n",
    "    next_sample()\n",
    "    calculate_output()\n",
    "    backpropagate()\n",
    "\n",
    "def mini_batch_gradient_descent(epochs: int, batch_size: int):\n",
    "    global weight_gradient, bias_gradient, index, weights, biases\n",
    "    index = 0\n",
    "    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "        batch_weights = [np.zeros_like(layer) for layer in weight_gradient]\n",
    "        batch_biases = [np.zeros_like(layer) for layer in bias_gradient]\n",
    "\n",
    "        missed = 0\n",
    "        for i in tqdm(range(batch_size), desc=\"Batch\", leave=False):\n",
    "            if i + index >= len(data):\n",
    "                missed += 1\n",
    "                break\n",
    "            learn()\n",
    "            batch_weights += weight_gradient\n",
    "            batch_biases += bias_gradient\n",
    "        \n",
    "        batch_weights = np.array([x / (batch_size - missed) for x in batch_weights],dtype=object)\n",
    "        for w1, w2 in zip(weights, batch_weights):\n",
    "            w1 -= w2\n",
    "\n",
    "        batch_biases = np.array([x / (batch_size - missed) for x in batch_biases],dtype=object)\n",
    "        for b1, b2 in zip(biases, batch_biases):\n",
    "            b1 -= b2\n",
    "        #weights -= [x / (batch_size - missed) if x is not None else None for x in batch_weights]\n",
    "        #biases -= [x / (batch_size - missed) if x is not None else None for x in batch_biases]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted data file already exists. Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Check if the formatted data file exists\n",
    "if os.path.exists(\"formatted_data.pkl\"):\n",
    "    print(\"Formatted data file already exists. Loading data...\")\n",
    "    # Load the formatted data from the file\n",
    "    with open(\"formatted_data.pkl\", \"rb\") as file:\n",
    "        data = pickle.load(file)\n",
    "else:\n",
    "    print(\"Formatted data file does not exist. Formatting data...\")\n",
    "    # Your existing code for formatting the data\n",
    "    \n",
    "    with open(\"mnist_train.csv\", \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        data = []\n",
    "        total_lines = len(lines)\n",
    "        for line in tqdm(lines, total=total_lines, desc=\"Processing data\"):\n",
    "            values = line.strip().split(\",\")\n",
    "            label = int(values[0])\n",
    "            image = np.array([float(x)/255 for x in values[1:]], dtype=np.uint8)\n",
    "            if len(image) != 784: print(len(image))\n",
    "            expected_output = np.zeros(10)\n",
    "            expected_output[label] = 1\n",
    "            example = [image, expected_output]\n",
    "            data.append(example)\n",
    "\n",
    "        # Save the training data into a file\n",
    "        with open(\"formatted_data.pkl\", \"wb\") as file:\n",
    "            pickle.dump(data, file)\n",
    "        print(\"Training data saved into formatted data file.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/200 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]])]\n",
      "[array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8), None, array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]])]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 19\u001b[0m\n\u001b[0;32m     14\u001b[0m bias_gradient \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(bias_gradient,dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#data = [[input, expected_output]]\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[43mmini_batch_gradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[55], line 18\u001b[0m, in \u001b[0;36mmini_batch_gradient_descent\u001b[1;34m(epochs, batch_size)\u001b[0m\n\u001b[0;32m     16\u001b[0m     missed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m batch_weights \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m weight_gradient\n\u001b[0;32m     20\u001b[0m batch_biases \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bias_gradient\n",
      "Cell \u001b[1;32mIn[55], line 3\u001b[0m, in \u001b[0;36mlearn\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m ():\n\u001b[0;32m      2\u001b[0m     next_sample()\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mcalculate_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     backpropagate()\n",
      "Cell \u001b[1;32mIn[49], line 15\u001b[0m, in \u001b[0;36mcalculate_output\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m layers, activations\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(layers)):\n\u001b[1;32m---> 15\u001b[0m     activations[i] \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[49], line 6\u001b[0m, in \u001b[0;36mcalculate_layer\u001b[1;34m(layer)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# z(L) = b(L) + W(L) * a(L-1)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(activations)\n\u001b[1;32m----> 6\u001b[0m weighted_input \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39madd(biases[layer], \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      7\u001b[0m weighted_inputs[layer] \u001b[38;5;241m=\u001b[39m weighted_input\n\u001b[0;32m      8\u001b[0m activations[layer] \u001b[38;5;241m=\u001b[39m vector_sigmoid(weighted_input)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "layers = [28*28,16,16,10]\n",
    "\n",
    "weights = [0] + [np.random.uniform(-1,1,size=(layers[i], layers[i-1])) for i in range(1, len(layers))]\n",
    "weigths = np.array(weights,dtype=object)\n",
    "biases = [0] + [np.random.uniform(-1, 1, size=(layers[i], 1)) for i in range(1, len(layers))]\n",
    "biases = np.array(biases,dtype=object)\n",
    "weighted_inputs = [0] + [np.zeros((layers[i], 1)) for i in range(1, len(layers))]\n",
    "weighted_inputs = np.array(weighted_inputs,dtype=object)\n",
    "\n",
    "activations = [np.zeros((layers[i], 1)) for i in range(0, len(layers))]\n",
    "weight_gradient = [np.zeros_like(layer) for layer in weights]\n",
    "weight_gradient = np.array(weight_gradient,dtype=object)\n",
    "bias_gradient = [np.zeros_like(layer) for layer in biases]\n",
    "bias_gradient = np.array(bias_gradient,dtype=object)\n",
    "\n",
    "#data = [[input, expected_output]]\n",
    "\n",
    "\n",
    "mini_batch_gradient_descent(200,80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
