{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import ctypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevents the screen saver, sleep, and monitor power off\n",
    "ES_CONTINUOUS = 0x80000000\n",
    "ES_SYSTEM_REQUIRED = 0x00000001\n",
    "\n",
    "def prevent_sleep():\n",
    "    ctypes.windll.kernel32.SetThreadExecutionState(ES_CONTINUOUS | ES_SYSTEM_REQUIRED)\n",
    "\n",
    "def allow_sleep():\n",
    "    ctypes.windll.kernel32.SetThreadExecutionState(ES_CONTINUOUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My activation function of choice. Sigmoid returns a value between 1 and 0.\n",
    "def sigmoid(x) -> float:\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Applies the sigmoid function to every value in the vector.\n",
    "def vector_sigmoid(vector) -> np.ndarray:\n",
    "    sig_vector = np.vectorize(sigmoid)(vector)\n",
    "    return sig_vector\n",
    "\n",
    "# Derivative of the sigmoid function.\n",
    "def sigmoid_derivative(x) -> float:\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the values of a single layer.\n",
    "def calculate_layer(layer : int) -> None:\n",
    "    global weights, biases, activations, weighted_inputs\n",
    "    # z(L) = b(L) + W(L) * a(L-1)\n",
    "    #print(\"biases[layer]: \", biases[layer])\n",
    "    weighted_input = biases[layer] + (weights[layer] @ activations[layer-1])\n",
    "    \n",
    "    weighted_inputs[layer] = weighted_input\n",
    "    activated = vector_sigmoid(weighted_input)\n",
    "    activations[layer] = activated\n",
    "\n",
    "\n",
    "# Loops through every layer and calculates their activation values.\n",
    "def calculate_output() -> None:\n",
    "    global layers, activations\n",
    "    for i in range(1,len(layers)):\n",
    "        calculate_layer(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the derivative of the chain rule.\n",
    "def calculate_chain_derivative(is_parent, layer, row, column) -> float:\n",
    "    global weights, weighted_inputs, activations, expected_output\n",
    "    sum = 0\n",
    "    if layer == len(activations) - 1:\n",
    "        cost_derivative = 2 * (activations[layer][row] - expected_output[row])\n",
    "        \n",
    "        sum = cost_derivative\n",
    "    else:\n",
    "        for i in range(len(activations[layer+1])):\n",
    "            sum += calculate_chain_derivative(False, layer + 1, i, row)\n",
    "    weighted_input = weighted_inputs[layer][row]\n",
    "    activation_derivative = sigmoid_derivative(weighted_input)\n",
    "    if is_parent:\n",
    "        return sum * activation_derivative\n",
    "    else:\n",
    "        weighted_input_derivative = weights[layer][row][column]\n",
    "        return sum * activation_derivative * weighted_input_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the derivative of the bias.\n",
    "def calculate_bias_derivative(is_parent, layer, row, column) -> float: # Literally just a useless function.\n",
    "    chain_derivative = calculate_chain_derivative(is_parent, layer, row, column)\n",
    "    return chain_derivative\n",
    "\n",
    "# Calculates the derivative of the weight.\n",
    "def calculate_weight_derivative(is_parent, layer, row, column) -> float:  \n",
    "    chain_derivative = calculate_chain_derivative(is_parent, layer, row, column)\n",
    "    result = chain_derivative * activations[layer-1][column]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backpropagate() -> None:\n",
    "    global weights, biases, activations, weighted_inputs, expected_output, weight_gradient, bias_gradient\n",
    "\n",
    "    for layer in range(1, len(weights)):\n",
    "        for row in range(len(weights[layer])):\n",
    "            for column in range(len(weights[layer][row])):\n",
    "                weight_gradient[layer][row][column] = calculate_weight_derivative(True, layer, row, column)\n",
    "    \n",
    "    for layer in range(1, len(biases)):\n",
    "        for row in range(len(biases[layer])):\n",
    "            bias_gradient[layer][row] = calculate_bias_derivative(True, layer, row, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_input(input : np.ndarray) -> None:\n",
    "    global activations\n",
    "    activations[0] = input\n",
    "\n",
    "def set_expected_output(output : np.ndarray) -> None:\n",
    "    global expected_output\n",
    "    expected_output = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_sample():\n",
    "    global index, data\n",
    "    #print(\"Index: \", index)\n",
    "    sample = data[index]\n",
    "    set_input(sample[0])\n",
    "    set_expected_output(sample[1])\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn() -> None:\n",
    "    next_sample()\n",
    "    calculate_output()\n",
    "    backpropagate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_network() -> None:\n",
    "    global weights, biases\n",
    "    with open(\"weights.pkl\", \"wb\") as file:\n",
    "        pickle.dump(weights, file)\n",
    "    with open(\"biases.pkl\", \"wb\") as file:\n",
    "        pickle.dump(biases, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make it loop around the test data if it reaches the end.\n",
    "def mini_batch_gradient_descent(epochs: int, batch_size: int, lr: float, save_frequency: int) -> None:\n",
    "    prevent_sleep()\n",
    "    global weight_gradient, bias_gradient, index, weights, biases\n",
    "    index = 0\n",
    "    for epoch in tqdm(range(epochs), desc=\"Epoch\", leave=None):\n",
    "        batch_weights = [np.zeros_like(layer) for layer in weight_gradient]\n",
    "        batch_weights = np.array(batch_weights, dtype=object)\n",
    "        batch_biases = [np.zeros_like(layer) for layer in bias_gradient]\n",
    "        batch_biases = np.array(batch_biases, dtype=object)\n",
    "\n",
    "        missed = 0\n",
    "        for i in range(batch_size):\n",
    "            if i + index >= len(data):\n",
    "                missed += 1\n",
    "                break\n",
    "            learn()\n",
    "            #print(\"batch_weights: \", batch_weights)\n",
    "            #print(\"weight_gradient: \", weight_gradient)\n",
    "            batch_weights += weight_gradient\n",
    "            batch_biases += bias_gradient\n",
    "        \n",
    "        batch_weights = np.array([x / (batch_size - missed) for x in batch_weights],dtype=object)\n",
    "        #weights -= batch_weights\n",
    "        for w, b in zip(weights, batch_weights):\n",
    "            w -= b * lr\n",
    "        \n",
    "        \n",
    "        batch_biases = np.array([x / (batch_size - missed) for x in batch_biases],dtype=object)\n",
    "        #biases -= batch_biases\n",
    "        for b, b2 in zip(biases, batch_biases):\n",
    "            b -= b2 * lr\n",
    "\n",
    "        if epoch % save_frequency == 0:\n",
    "            save_network()\n",
    "    allow_sleep()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted data file already exists. Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 1/1 [00:00<00:00,  2.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# Check if the formatted data file exists\n",
    "if os.path.exists(\"formatted_data.pkl\"):\n",
    "    print(\"Formatted data file already exists. Loading data...\")\n",
    "    with tqdm(total=1, desc=\"Loading data\") as pbar:\n",
    "        with open(\"formatted_data.pkl\", \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        pbar.update(1)\n",
    "else:\n",
    "    print(\"Formatted data file does not exist. Formatting data...\")\n",
    "    # Your existing code for formatting the data\n",
    "    \n",
    "    with open(\"mnist_train.csv\", \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        data = []\n",
    "        total_lines = len(lines)\n",
    "        for line in tqdm(lines, total=total_lines, desc=\"Processing data\"):\n",
    "            values = line.strip().split(\",\")\n",
    "            label = int(values[0])\n",
    "            image = np.array([float(x)/255 for x in values[1:]], dtype=np.float32)\n",
    "            if len(image) != 784: print(\"Error! Image wrong size: \" + len(image))\n",
    "            expected_output = np.zeros(10)\n",
    "            expected_output[label] = 1\n",
    "            example = [image, expected_output]\n",
    "            data.append(example)\n",
    "\n",
    "        # Save the training data into a file\n",
    "        with open(\"formatted_data.pkl\", \"wb\") as file:\n",
    "            pickle.dump(data, file)\n",
    "        print(\"Training data saved into formatted data file.\")\n",
    "\n",
    "\n",
    "#print(data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 200/200 [9:31:43<00:00, 171.52s/it]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights and biases saved into files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Turn off dumb warnings\n",
    "#np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
    "layers = [784, 16, 16, 10]\n",
    "\n",
    "weights = [0] + [np.random.uniform(-1,1,size=(layers[i], layers[i-1])) for i in range(1, len(layers))]\n",
    "weigths = np.array(weights,dtype=object)\n",
    "\n",
    "biases = [0] + [np.random.uniform(-1, 1, size=layers[i]) for i in range(1, len(layers))]\n",
    "biases = np.array(biases, dtype=object)\n",
    "\n",
    "weighted_inputs = [0] + [np.zeros((layers[i], 1)) for i in range(1, len(layers))]\n",
    "weighted_inputs = np.array(weighted_inputs, dtype=object)\n",
    "\n",
    "activations = [np.zeros((layers[i], 1)) for i in range(0, len(layers))]\n",
    "weight_gradient = [np.zeros_like(layer) for layer in weights]\n",
    "weight_gradient = np.array(weight_gradient, dtype=object)\n",
    "bias_gradient = [np.zeros_like(layer) for layer in biases]\n",
    "bias_gradient = np.array(bias_gradient, dtype=object)\n",
    "\n",
    "mini_batch_gradient_descent(200, 10, 0.005)\n",
    "# If you get a divide by zero error, it's probably because the batch size and epochs is too large.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the NN has been trained, we can test its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test() -> None:\n",
    "    global activations, test_data\n",
    "    correct = 0\n",
    "    for sample in test_data:\n",
    "        set_input(sample[0])\n",
    "        set_expected_output(sample[1])\n",
    "        calculate_output()\n",
    "        output = activations[-1]\n",
    "        if np.argmax(output) == np.argmax(sample[1]):\n",
    "            correct += 1\n",
    "    print(\"Correct: \", correct, \"/\", len(test_data))\n",
    "    print(\"Accuracy: \", correct/len(test_data)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted test data file does not exist. Formatting data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 10000/10000 [00:01<00:00, 5092.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data saved into formatted test data file.\n"
     ]
    }
   ],
   "source": [
    "# Check if the formatted data file exists\n",
    "if os.path.exists(\"formatted_test_data.pkl\"):\n",
    "    print(\"Formatted test data file already exists. Loading data...\")\n",
    "    with tqdm(total=1, desc=\"Loading data\") as pbar:\n",
    "        with open(\"formatted_test_data.pkl\", \"rb\") as file:\n",
    "            test_data = pickle.load(file)\n",
    "        pbar.update(1)\n",
    "else:\n",
    "    print(\"Formatted test data file does not exist. Formatting data...\")\n",
    "    # Your existing code for formatting the data\n",
    "    \n",
    "    with open(\"mnist_test.csv\", \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        test_data = []\n",
    "        total_lines = len(lines)\n",
    "        for line in tqdm(lines, total=total_lines, desc=\"Processing data\"):\n",
    "            values = line.strip().split(\",\")\n",
    "            label = int(values[0])\n",
    "            image = np.array([float(x)/255 for x in values[1:]], dtype=np.float32)\n",
    "            if len(image) != 784: print(\"Error! Image wrong size: \" + len(image))\n",
    "            expected_output = np.zeros(10)\n",
    "            expected_output[label] = 1\n",
    "            example = [image, expected_output]\n",
    "            test_data.append(example)\n",
    "\n",
    "        # Save the training data into a file\n",
    "        with open(\"formatted_test_data.pkl\", \"wb\") as file:\n",
    "            pickle.dump(test_data, file)\n",
    "        print(\"Training data saved into formatted test data file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:  1046 / 10000\n",
      "Accuracy:  10.459999999999999 %\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, array([[ 0.06937077, -0.84508992,  0.38098064, ..., -0.29113902,\n",
      "        -0.16481954, -0.61047532],\n",
      "       [-0.53709845,  0.02162415,  0.32540907, ...,  0.05456196,\n",
      "         0.45044997, -0.73815714],\n",
      "       [ 0.29402003, -0.84784307,  0.18087987, ...,  0.53424392,\n",
      "        -0.76786185,  0.28611547],\n",
      "       ...,\n",
      "       [ 0.44055076,  0.84104061,  0.50117378, ...,  0.2252591 ,\n",
      "        -0.18963522,  0.07030325],\n",
      "       [-0.73396905,  0.82878698,  0.28206588, ...,  0.40450718,\n",
      "        -0.07328773,  0.00426636],\n",
      "       [ 0.61512547, -0.60952177,  0.01999299, ..., -0.54131904,\n",
      "        -0.35026074,  0.03996377]]), array([[ 0.41216561, -0.26690612,  0.71384004,  0.07303268,  0.5756867 ,\n",
      "         0.8181677 ,  0.09272674,  0.83188007,  0.2563646 , -0.05931308,\n",
      "         0.5741621 , -0.11587914, -0.93894792,  0.05942882,  0.26541549,\n",
      "         0.97803597],\n",
      "       [-0.62014687,  0.97267346,  0.56770552, -0.61508529,  0.69505902,\n",
      "        -0.33561932, -0.28380962,  0.22250728, -0.83866526,  0.36143807,\n",
      "        -0.49268352,  0.30077789, -0.81294709,  0.87444267, -0.92316978,\n",
      "         0.53031563],\n",
      "       [ 0.77164046, -0.9066107 ,  0.86746448,  0.212809  , -0.50075471,\n",
      "        -0.57929562,  0.19896885,  0.18199082,  0.94848387, -0.16775002,\n",
      "        -0.91101665, -0.92153252, -0.78029617, -0.34066565, -0.14092975,\n",
      "        -0.0649474 ],\n",
      "       [-0.88830644, -0.08133275,  0.19419055, -0.75743861,  0.8263918 ,\n",
      "        -0.80625773,  0.00720632, -0.91793107,  0.68975744,  0.98196893,\n",
      "         0.16955303, -0.42292389, -0.61936975, -0.09723223,  0.63804889,\n",
      "         0.63807675],\n",
      "       [-0.44525317, -0.81615293,  0.69772536, -0.44791987,  0.44026994,\n",
      "        -0.06526094,  0.71940076,  0.56356796, -0.72193423,  0.77244383,\n",
      "         0.52529805, -0.87434467,  0.68467288,  1.02106554, -0.6817842 ,\n",
      "        -0.05248919],\n",
      "       [-0.76707157, -0.87017855, -0.6209818 ,  0.23328095, -0.20167951,\n",
      "         0.69293344,  0.28756765, -0.60888573, -0.99596841,  0.39093551,\n",
      "         0.61623467,  0.06250272,  0.97611145,  0.0779139 , -0.071777  ,\n",
      "        -0.10029167],\n",
      "       [-0.43016778,  0.51467365,  0.01570752, -0.42739972,  0.47241539,\n",
      "         0.70593553, -0.30288408, -0.06835941,  0.46163868,  0.3656079 ,\n",
      "         0.63450824, -0.30362157,  0.48441159, -0.45793911, -0.96001731,\n",
      "         0.59105849],\n",
      "       [-0.62262223,  0.12144489, -0.58832103,  0.58177076,  0.82442776,\n",
      "        -0.96651981, -0.48767105,  0.34544287,  0.15596699, -0.95046217,\n",
      "        -0.46507489, -0.9712128 ,  0.99236353,  0.2261909 , -0.79550831,\n",
      "        -0.08335806],\n",
      "       [ 0.00799111,  0.56686198,  0.23208086,  0.66340068, -0.76981661,\n",
      "         0.70234938,  0.54017088, -0.30379577, -0.69867676, -0.68995221,\n",
      "         0.33023909,  0.90274922,  0.45012588,  0.39394908, -0.8367587 ,\n",
      "         0.32894015],\n",
      "       [ 0.75752027, -0.57183356,  0.49849037, -0.43956734,  0.36128216,\n",
      "        -0.53538001, -0.07141286, -0.97125773, -0.7104899 , -0.66116734,\n",
      "        -0.63494093, -0.86856619,  0.4201668 ,  0.66169138,  0.08461243,\n",
      "         0.10318318],\n",
      "       [-0.29773285, -0.79281961, -0.2481836 ,  0.86979579,  0.61116679,\n",
      "         0.68885296,  0.44428812, -0.05661661,  0.2734262 ,  0.33443106,\n",
      "        -0.37940716,  0.42283941, -0.51885002,  0.44919104, -0.9088373 ,\n",
      "        -0.97368535],\n",
      "       [-0.95740026, -0.3724696 ,  0.0800575 ,  0.57249904,  0.71690339,\n",
      "        -0.55289029, -0.53830405, -0.61217868, -0.62877964,  0.43651862,\n",
      "        -0.54234735, -0.70008161,  1.01257876, -0.51880729,  0.95101886,\n",
      "        -0.3575094 ],\n",
      "       [-0.62585112, -0.3378669 , -0.20964902, -0.2835045 ,  0.71562356,\n",
      "        -0.8190101 , -0.72896796,  0.2230446 ,  0.41276233, -0.6240923 ,\n",
      "        -0.29932885, -0.67285046,  0.48330284, -0.6725558 , -0.71536887,\n",
      "        -0.5878993 ],\n",
      "       [-0.70737451,  0.56678907, -0.97691608,  0.7211013 , -0.74165848,\n",
      "        -0.97000317,  0.03082548,  0.13879285,  0.64049863,  0.59371955,\n",
      "        -0.73812974,  0.03349205, -0.03179838, -0.1194031 ,  0.1794832 ,\n",
      "         0.62690624],\n",
      "       [-0.8683166 ,  0.95784711, -0.26948262,  0.48321581,  0.37144797,\n",
      "        -0.16825425, -0.1619978 , -0.99991842, -0.4965302 ,  0.67035587,\n",
      "        -0.78275133,  0.73120234, -0.83901088, -0.55264097,  0.16862957,\n",
      "         0.5052969 ],\n",
      "       [-0.6807813 , -0.9181818 , -0.13427607,  0.47984175, -0.54084434,\n",
      "        -0.01428692, -0.70609551, -0.27287805, -0.53378965, -0.36920824,\n",
      "         0.23381912,  0.15508158,  0.41716133,  0.59689722, -0.13794002,\n",
      "         0.55258362]]), array([[ 0.35915248, -0.651402  ,  0.40182554,  0.81509686, -0.5560039 ,\n",
      "        -0.45919   , -0.48411257, -0.284716  ,  0.10746134,  0.84232935,\n",
      "        -0.99293436, -0.88006881, -0.00460494, -0.83764131,  0.72054412,\n",
      "        -0.12096162],\n",
      "       [-1.06075572, -0.16007076, -0.84083672,  0.39010326, -0.62649029,\n",
      "         0.01181753,  0.7564378 ,  0.40558043,  0.31340055,  0.38691319,\n",
      "         0.70969056, -0.78233625,  0.24779263,  0.90550637, -0.07019926,\n",
      "        -0.66943771],\n",
      "       [-0.07178474,  0.12169936,  0.46929241, -0.63762961,  0.88763678,\n",
      "         0.11022996,  0.68058415,  0.38764785, -0.79436398, -0.13034095,\n",
      "        -0.42830114, -0.16815347,  0.15067774, -0.78934479,  0.26853793,\n",
      "        -0.21770232],\n",
      "       [-0.83112523,  0.6955776 , -0.44102531, -0.98399372, -0.54599521,\n",
      "         0.89175249,  0.52020292,  0.09973057, -0.33807751, -0.9813767 ,\n",
      "         0.10039268, -0.93259588, -0.58308133, -0.20157436, -0.49944149,\n",
      "        -0.17312813],\n",
      "       [-0.66198554, -0.91660505,  0.3971525 , -0.45242955, -0.69434401,\n",
      "         0.66833301,  0.83166563, -0.42485707, -0.77385652,  0.23872028,\n",
      "        -0.29769542,  0.6868601 ,  0.89020647,  0.10006358, -0.07558856,\n",
      "         0.20681843],\n",
      "       [-0.83501103, -0.83966736, -0.15115907, -0.03247219,  0.71229565,\n",
      "        -0.56978122, -0.18363973, -0.24478922,  0.83289066,  0.50472717,\n",
      "         0.72663274, -0.32932508,  0.20458344, -0.30214406, -0.64251947,\n",
      "        -0.78005622],\n",
      "       [ 0.6017635 , -0.26393245, -0.57681397, -0.15872439,  0.15200617,\n",
      "        -0.32285126, -1.12642708, -0.69927388,  0.11941211,  0.45729983,\n",
      "         0.41225688,  0.20702775, -0.77198609,  0.04282728,  0.86762661,\n",
      "        -0.39872029],\n",
      "       [-0.09636288,  0.53262745, -0.58526823,  0.70857131, -1.06426384,\n",
      "         0.55151954, -0.12905183, -0.99421098, -0.29237633, -0.94242082,\n",
      "        -0.38845419, -0.70671692,  0.68296742, -0.80404812, -0.07292003,\n",
      "         0.8909332 ],\n",
      "       [-0.67068146, -0.66294775, -0.02162597, -0.87328089, -0.84748036,\n",
      "        -0.67289325, -0.1831749 ,  0.92899122,  0.28867433, -0.3159839 ,\n",
      "         0.49677522,  0.41300505,  0.77945285, -0.71651918, -0.3843045 ,\n",
      "        -0.80297003],\n",
      "       [ 0.85224619, -0.66211423, -0.1745659 ,  0.10126088, -0.90173638,\n",
      "        -0.39044454,  0.05816622, -0.50203425,  0.13239837,  0.23098226,\n",
      "        -0.65701111, -0.46908902, -0.17534483,  0.72405105,  0.05517225,\n",
      "         0.52714415]])]\n"
     ]
    }
   ],
   "source": [
    "print(weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
