{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self) -> None:\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        pass\n",
    "\n",
    "    def forward(self, input):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, output_derivative, lr):\n",
    "        pass\n",
    "\n",
    "    def update(self, lr):\n",
    "        self.weights -= lr * self.grad_weights\n",
    "        self.biases -= lr * self.grad_biases\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullLayer(Layer):\n",
    "    def __init__(self, input_size, output_size) -> None:\n",
    "        self.weights = np.random.randn(input_size, output_size)\n",
    "        self.biases = np.random.randn(output_size)\n",
    "        self.grad_weights = np.zeros_like(self.weights)\n",
    "        self.grad_biases = np.zeros_like(self.biases)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        print (input.shape, self.weights.shape)\n",
    "        self.output = np.dot(input, self.weights) + self.biases\n",
    "        print(self.biases.shape)\n",
    "        print (self.output.shape)\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, cost_derivatives):\n",
    "        weighted_sum_derivatives = self.input\n",
    "        activation_derivatives = self.sigmoid_derivative(self.output)\n",
    "        #print (cost_derivatives.shape, self.input.shape)\n",
    "        self.grad_biases = cost_derivatives * activation_derivatives\n",
    "        \n",
    "        #TODO: check this\n",
    "        self.grad_weigts = np.dot(cost_derivatives, self.input.T)\n",
    "    \n",
    "        #TODO: check this\n",
    "        return cost_derivatives * activation_derivatives * weighted_sum_derivatives\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_weight(kernel_size):\n",
    "    return np.random.randn(kernel_size)\n",
    "\n",
    "def kernel_bias(input_size, kernel_size):\n",
    "    input_size = input_size[:2]\n",
    "    kernel_size = kernel_size[:2]\n",
    "    output_size = tuple(map(lambda i, j: i - j + 1, input_size, kernel_size)) # Valid padding\n",
    "    return np.random.randn(output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleConvLayer(Layer): # Valid padding\n",
    "    def __init__ (self, kernel_sizes : list, input_size : tuple) -> None:\n",
    "        self.weigths = [kernel_weight(kernel_size) for kernel_size in kernel_sizes]\n",
    "        self.biases = [kernel_bias(input_size, kernel_size) for kernel_size in kernel_sizes]\n",
    "        #self.weighted_sum = None\n",
    "        self.grad_weights = np.zeros_like(self.weights)\n",
    "        self.grad_biases = np.zeros_like(self.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = [np.correlate(input, kernel) + bias for kernel, bias in zip(self.kernels_weights, self.kernels_bias)]\n",
    "        return self.output\n",
    "    \n",
    "    #def backward(self, cost_derivatives):\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self,layers : list) -> None: \n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, inputs : np.ndarray) -> np.ndarray:\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.forward(inputs)\n",
    "        return inputs\n",
    "    \n",
    "    def backward(self, output : np.ndarray, desired_output : np.ndarray) -> None:\n",
    "        cost_derivatives = 2 * (output - desired_output)\n",
    "        for layer in reversed(self.layers):\n",
    "            cost_derivatives = layer.backward(cost_derivatives)\n",
    "            # Derivatives with respect to trainable parameters are saved in indivisual layers\n",
    "    \n",
    "    def update(self, lr : float) -> None:\n",
    "        for layer in self.layers:\n",
    "            layer.update(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,) (10, 7)\n",
      "(7,)\n",
      "(7,)\n",
      "(7,) (7,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (7,) and (10,) not aligned: 7 (dim 0) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[170], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m desired_output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m7\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape, desired_output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesired_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[169], line 13\u001b[0m, in \u001b[0;36mNetwork.backward\u001b[1;34m(self, output, desired_output)\u001b[0m\n\u001b[0;32m     11\u001b[0m cost_derivatives \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (output \u001b[38;5;241m-\u001b[39m desired_output)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m---> 13\u001b[0m     cost_derivatives \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcost_derivatives\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[166], line 23\u001b[0m, in \u001b[0;36mFullLayer.backward\u001b[1;34m(self, cost_derivatives)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_biases \u001b[38;5;241m=\u001b[39m cost_derivatives \u001b[38;5;241m*\u001b[39m activation_derivatives\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#TODO: check this\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_weigts \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcost_derivatives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#TODO: check this\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cost_derivatives \u001b[38;5;241m*\u001b[39m activation_derivatives \u001b[38;5;241m*\u001b[39m weighted_sum_derivatives\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (7,) and (10,) not aligned: 7 (dim 0) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Define structure of the network\n",
    "network = Network(\n",
    "    [\n",
    "        FullLayer(10, 7)\n",
    "    ]\n",
    ")\n",
    "\n",
    "output = network.forward(np.random.randn(10))\n",
    "desired_output = np.random.randn(7)\n",
    "print(output.shape, desired_output.shape)\n",
    "network.backward(output, desired_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
