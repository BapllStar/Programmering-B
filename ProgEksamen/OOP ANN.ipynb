{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "I dette projekt vil jeg lave et objectorienteret ANN, der skal kunne genkende forskellige håndskrevne cifre og sortere dem i kategorier.\n",
    "Jeg har valgt at lave dem objektorienteret, da jeg før har forsøgt at lave ANN'er før, hvor det var baseret hovedsagligt ud fra funktioner, hvilket ledte til, at de hele blev meget uoverskueligt og repeterivt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importering af biblioteker\n",
    "Først vil jeg importere nogle biblioteker med foruddefinerede funktioner, der kan hjælpe med at gøre udviklingsprocessen nemmere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Matricer og vektorer\n",
    "np.random.seed(2) # For at kunne genskabe de samme resultater.\n",
    "\n",
    "import pandas as pd # Til at læse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False # Skal der printes debug information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defineringen af et lag\n",
    "Først vil jeg definere en superklasse, lag, som de andre typer af lag skal nedarve fra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lag:\n",
    "    # __init__, også kendt som constructor, er kort for 'intialize', og \n",
    "    # er en metode, der kaldes, når et objekt af klassen Lag oprettes. \n",
    "    # Init har ikke noget output.\n",
    "    def __init__(self) -> None:\n",
    "        # Input- of outputtypen afhænger af, hvilekt slags lag det er. \n",
    "        # Men alle lag har in og out, så jeg definerer dem som None her, \n",
    "        # for at de er der, når jeg nedarver herfra.\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    # I frem-metoden, udregner vi outputtet ud fra inputtet. Hvad der sker \n",
    "    # i frem(), afhænger af, hvilket slags lag det er.\n",
    "    def frem(self, input):\n",
    "        pass\n",
    "\n",
    "    # I tilbage-metoden, udregner vi hvilke ændringer, der skal foretages \n",
    "    # på de trænede vægte og biaser, ud fra outputtet.\n",
    "    def tilbage(self, pa_output): \n",
    "        # 'pa_output' er 'partielle afledte af outputtet'. Her snakkes der\n",
    "        # om outputtet som blev udregnet i frem(). Dette er gives til laget,\n",
    "        # fra det næste lag i netværkert under bagudpropagering.\n",
    "        pass\n",
    "\n",
    "    def update(self, skridt_længde):\n",
    "        # Denne metode opdaterer vægtene og biaserne i laget, ud fra de \n",
    "        # ændringer, der blev udregnet i tilbage(). \n",
    "\n",
    "        # 'skridt_længde' er en hyperparameter, der bestemmer, hvor meget\n",
    "        # vi vil ændre på vægtene og biaserne. Denne værdi er fastsat af mig\n",
    "        # i dette tilfælde. Man kunne dog vælge at få den til at variere, alt\n",
    "        # efter hvor langt netværket er fra sit mål.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definering af FF_Lag\n",
    "Et FF-lag, eller et 'Fuldt forbundet' lag, er en type af lag, der ofte findes i Artificial Neural Networks. Her består et lag af neuroner, der er forbundet med synapser til alle de kommende lag. Her undlader jeg dog at bruge en aktiveringsfunktion, da dette kan gøres i et separat lag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dette er et fuldt forbundet lag, også kendt som et fully connected layer, eller dense layer.\n",
    "class FF(Lag):\n",
    "    def __init__(self, n_input : int, n_output : int) -> None:\n",
    "        if debug: print(\"FF init\")\n",
    "\n",
    "        # Her er n_input antallet af neuroner i det forrige lag, og n_output\n",
    "        # er antallet af neuroner i det næste lag.\n",
    "        # Dett skal jeg bruge for at vide, hvilket facon vægte og biaser skal have.\n",
    "        \n",
    "        self.w = np.random.randn(n_output, n_input).astype(np.float64)\n",
    "        # w er vægtene. De er initialiseret til at være tilfældige tal. Vægte er\n",
    "        # opbygget til at være samlet i en matrix, hvor højde er antaller af\n",
    "        # neuroner i det næste lag, og bredde er antallet af neuroner i det forrige lag.\n",
    "        # Vægte bliver ganget på inputtet i frem() metoden.\n",
    "        \n",
    "        self.b = np.random.randn(n_output, 1).astype(np.float64)\n",
    "        # b er biaserne. De er initialiseret til at være tilfældige tal. Biaser er\n",
    "        # opbygget til at være en vektor, hvor højden er antallet af neuroner\n",
    "        # i det næste lag.\n",
    "        # Biaser bliver lagt til vægtene i frem() metoden.\n",
    "\n",
    "        self.pa_w = np.zeros_like(self.w).astype(np.float64)\n",
    "        self.pa_b = np.zeros_like(self.b).astype(np.float64)\n",
    "        # Disse variabler er sat til None, da de først bliver brugt i tilbage() og update() metoden.\n",
    "\n",
    "        self.iterations = 0\n",
    "        pass\n",
    "    \n",
    "    def frem(self, input) -> np.ndarray:\n",
    "        if debug: print(\"FF frem\")\n",
    "\n",
    "        # Normalt ville man have en aktiveringsfunktion her, men dette kan også gøres\n",
    "        # som et separat lag. Derfor er det udeladt her for at gøre det mere fleksibelt.\n",
    "\n",
    "        vægtet = np.dot(self.w, input)\n",
    "        # Inputtet ganges med vægtene, for at udregne outputtet. Dette gøres ved at\n",
    "        # lave en matrix-transformation af inputvektoren.\n",
    "\n",
    "        \n",
    "        forskudt = vægtet + self.b\n",
    "        # Biasvektoren lægges til det vægtede input. Dette kan forskyde \n",
    "\n",
    "        if debug:\n",
    "            print(\"input\")\n",
    "            print(np.shape(input))\n",
    "            print(\"vægtet\")\n",
    "            print(np.shape(vægtet))\n",
    "            print(\"self.b\")\n",
    "            print(np.shape(self.b))\n",
    "            print(\"forskudt\")\n",
    "            print(np.shape(forskudt))\n",
    "\n",
    "\n",
    "        self.input = input\n",
    "        self.output = forskudt\n",
    "        # Inputtet gemmes, så det kan bruges i tilbage() metoden.\n",
    "\n",
    "        return forskudt\n",
    "\n",
    "    def tilbage(self, pa_output) -> np.ndarray:\n",
    "        if debug: print(\"FF tilbage\")\n",
    "        \n",
    "        self.pa_w += np.dot(pa_output, self.input.T)\n",
    "        if debug: print(\"w clear\")\n",
    "        # Hvis man differencerer frem() metoden med hensyn til vægtene, finder man at\n",
    "        # resultatet er inputtet ganget med partielle afledte af outputtet.\n",
    "        # Vi transponerer inputtet, da det er en kolonnevektor, så resultatet bliver \n",
    "        # en matrix, der passer til vægtenes facon.\n",
    "\n",
    "        self.pa_b += pa_output # * 1\n",
    "        if debug: print(\"b clear\")\n",
    "        # Da biasene kun adderes på, er den partielle afledte af biaserne lig med 1.\n",
    "\n",
    "        # Vi gemmer de partielle afledte, så de kan bruges i update() metoden. Grunden til at\n",
    "        # vi ikke opdaterer vægtene og biaserne med det samme, er at vi gerne vil køre mini-batches.\n",
    "        # Dette betyder at vi først vil køre en række input igennem, og derefter opdatere vægtene og biaserne.\n",
    "\n",
    "        pa_input = np.dot(self.w.T, pa_output)\n",
    "        if debug: print(\"input clear\")\n",
    "        # I denne metode har vi brugt pa_output fra det næste lag, til at udregne de ændringer,\n",
    "        # vi skal lave til vægtene og biaserne. Vi skal også bruge et pa_output til det forrige lag.\n",
    "        # Da inputtet ganges med vægtene, for at udregne outputtet, er den partielle afledte af inputtet\n",
    "        # lig med vægtene ganget med pa_output fra det næste lag. Vi transponerer vægtene, for at få\n",
    "        # de så matcher til outputtet.\n",
    "\n",
    "        if debug:\n",
    "            print(\"pa_output\")\n",
    "            print(np.shape(pa_output))\n",
    "            print(\"pa_input\")\n",
    "            print(np.shape(pa_input))\n",
    "\n",
    "        self.iterations += 1\n",
    "\n",
    "        return pa_input\n",
    "    \n",
    "    def update(self, skridt_længde):\n",
    "        if debug: print(\"FF update\")\n",
    "\n",
    "        self.w -= skridt_længde * (self.pa_w/self.iterations)\n",
    "        self.b -= skridt_længde * (self.pa_b/self.iterations)\n",
    "        # Vægtene og biaserne opdateres ved at trække den partielle afledte af vægtene\n",
    "        # Da vi vil gå mod minimum, fremfor maksimum, trækker vi fra, og ganger\n",
    "        # med skridtlængden.\n",
    "\n",
    "        self.pa_w = np.zeros_like(self.w).astype(np.float64)\n",
    "        self.pa_b = np.zeros_like(self.b).astype(np.float64)\n",
    "        # Vi nulstiller de partielle afledte, så de er klar til næste Epoke.\n",
    "\n",
    "        self.iterations = 0\n",
    "        # Vi nulstiller antallet af iterationer, så vi kan tælle op til næste Epoke.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definering af Aktiveringsfunktionslag\n",
    "Normal finder man aktiveringsfunktioner inde i neronerne, men man kan også vælge at lave dem til et separat lag. Så kan man holde kompleksiteten nede. Hellere flere simple lag, end få komplicerede.\n",
    "<br>Desuden er det alstå også bare pænere at kigge på..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Et lag, der bruger en aktiveringsfunktion. Dette lag har ingen vægte eller biaser.\n",
    "class Funktion(Lag):\n",
    "    def __init__(self, funktion, afledt_funktion) -> None:\n",
    "        if debug: print(\"Funktion init\")\n",
    "\n",
    "        # Funktion er en aktiveringsfunktion, og afledt_funktion er sjovt nok den afledte funktionen.\n",
    "        \n",
    "        self.funktion = funktion\n",
    "        self.afledt_funktion = afledt_funktion\n",
    "        # Her gemmes funktionerne, så de kan bruges i frem() og tilbage() metoderne.\n",
    "    \n",
    "    def frem(self, input) -> np.ndarray:\n",
    "        if debug: print(\"Funktion frem\")\n",
    "        \n",
    "        self.input = input\n",
    "        # Inputtet gemmes, så det kan bruges i tilbage() metoden.\n",
    "\n",
    "        ikke_lineært = self.funktion(input)\n",
    "        # Funktionen anvendes på inputtet, og outputtet returneres. Aktiveringsfunktioner bliver brugt\n",
    "        # til at give netværkets beslutningsbariere en ikke-lineæritet, da ellers ville kunne blive\n",
    "        # reduceret til en lineær transformation.\n",
    "        # Dette er vigtigt, da mange problemer ikke er lineært separable.\n",
    "\n",
    "        if debug: \n",
    "            print(\"input:\")\n",
    "            print(np.shape(input))\n",
    "\n",
    "            print(\"output:\")\n",
    "            print(np.shape(ikke_lineært))\n",
    "\n",
    "        return ikke_lineært\n",
    "    \n",
    "    def tilbage(self, pa_output) -> np.ndarray:\n",
    "        if debug: print(\"Funktion tilbage\")\n",
    "        # Da aktiveringsfunktioner ikke har nogle vægte eller biaser, er der ikke noget at opdatere.\n",
    "\n",
    "        pa_input = np.multiply(pa_output, self.afledt_funktion(self.input))\n",
    "        # Her tager vi Hadamard-produktet af pa_output og den afledte funktion af inputtet.\n",
    "        if debug: print(np.shape(pa_input))\n",
    "        return pa_input\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For specifikke aktiveringsfunktioner, kan vi nedarve fra Funktion, og give dem en specifik funktion og afledt funktion.\n",
    "# Her bruger jeg sigmoid, da den giver nogle pæne værdier mellem 0 og 1. Den er meget ligesom en tanH-funktion, der giver\n",
    "# værdier mellem -1 og 1. Dog kan jeg godt lide den pæne form, som sigmoid har.\n",
    "class Sigmoid(Funktion):\n",
    "    def __init__(self):\n",
    "\n",
    "        sigmoid = lambda x: 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "        afledt_sigmoid = lambda x: sigmoid(x) * (1 - sigmoid(x)) \n",
    "        # Her bruger jeg funktionelle lambda-udtryk, for at definere sigmoid og dens afledte funktion.\n",
    "        # Dette gør, at jeg kan gemme dem i en variabel, og give dem til super's constructor.\n",
    "        # Lamda-udtryk har det some regel også med at være lidt hurtigere end funktioner, men det\n",
    "        # betyder ikke meget her, fordi jeg ikke fokuserer på hastighed.\n",
    "\n",
    "        # I den afledte funktion, bruger jeg sigmoid(x) for at undgå at skulle udregne med for store tal.\n",
    "\n",
    "        super().__init__(sigmoid, afledt_sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udregning af netværkets tab\n",
    "Når vi skal træne det neurale netværk, har vi brug for at vide, hvor godt netværket klarer sig. Derfor skal vi bruge et udtryk, der hedder 'tab'. Dette kan man finde ved at sammenligne netværkets output med éns ønskede output.\n",
    "$$\n",
    "Tab = \n",
    "(Output - ØnsketOutput)^2\n",
    "$$\n",
    "Grunden til at man sætter det i anden, er fordi programmet går efter mindst muligt tab. Hvis denne værdi kunne gå i negativ, fordi det ønskede output var højere end outputtet, ville programmet blive ved med at bevæge outputtet længere væk fra det ønskede output, fordi det ville blive en lavere værdi. Dertil får man den bonus, at tabet bliver eksponintielt voldsommere, jo mere forkert outputtet er."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ikke brugbar til træning, da man kun skal bruge den afledte funktion til at køre bagudpropageringen.\n",
    "# Dog kan man bruge denne her til at holde øje med, om netværket lærer noget.\n",
    "def tab(output, ønsket_output) -> float:\n",
    "    # Tab er en funktion, der udregner hvor langt netværket er fra sit mål.\n",
    "    return np.sum((output - ønsket_output) ** 2)\n",
    "\n",
    "def pa_tab(output, ønsket_output) -> np.ndarray:\n",
    "    # Den partielle afledte af tab-funktionen med hensyn til outputtet.\n",
    "    if debug:\n",
    "        print (\"pa_tab\")\n",
    "        print(np.shape(output))\n",
    "        print(np.shape(ønsket_output))\n",
    "    return 2 * (output - ønsket_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definering af et Netværk\n",
    "Normalt ville man ikke behøve at definere en klasse til at køre alle udregningerne, men da jeg gerne vi kunne lave flere versioner af forskellige netværk til tests med mere, er det hurtigere at samle det til en klasse, så jeg kan bruge den på forskellige måder, uden at skulle genskrive al koden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Netværk():\n",
    "    def __init__(self, lag) -> None:\n",
    "        self.lag = lag\n",
    "        # Lag er en liste af lag, som netværket består af.\n",
    "\n",
    "    def frem(self, input) -> np.ndarray:\n",
    "        # Frem-metoden tager inputtet, og sender det igennem alle lagene i netværket.\n",
    "\n",
    "        for l in self.lag:\n",
    "            input = l.frem(input)\n",
    "            # Inputtet sendes igennem hvert lag, og outputtet bliver det nye input.\n",
    "        return input\n",
    "    \n",
    "    def __tilbage(self, pa_output) -> np.ndarray:\n",
    "        # Tilbage-metoden tager partielle afledte af outputtet, og sender dem igennem alle lagene i netværket.\n",
    "\n",
    "        for l in reversed(self.lag):\n",
    "            pa_output = l.tilbage(pa_output)\n",
    "            # Partielle afledte af outputtet sendes igennem hvert lag, og outputtet bliver det nye input.\n",
    "        return pa_output\n",
    "    \n",
    "    def __update(self, skridt_længde) -> None:\n",
    "        # Update-metoden opdaterer vægtene og biaserne i alle lagene i netværket.\n",
    "\n",
    "        for l in self.lag:\n",
    "            l.update(skridt_længde)\n",
    "            # Vægtene og biaserne i hvert lag opdateres.\n",
    "    \n",
    "    def __tab(self, output, ønsket_output) -> float:\n",
    "        # Tab-metoden tager outputtet og ønsket_outputtet, og udregner tabet.\n",
    "        # Dette er udelukkende, så vi kan se, hvor langt netværket er fra sit mål.\n",
    "        return tab(output, ønsket_output)\n",
    "    \n",
    "\n",
    "    # Jeg har valgt at lave de overstående metoder private, da de ikke skal bruges udenfor klassen.\n",
    "\n",
    "\n",
    "    def træn(self, inputs : np.ndarray, ønskede_outputs : np.ndarray, skridt_længde : float, epoker : int, batchstørrelse : int, tab_print_interval : int) -> None:\n",
    "        # Træn-metoden tager inputs, ønskede_outputs, skridt_længde, antallet af epoker og batchstørrelsen.\n",
    "        # Den træner netværket, ved at køre inputs igennem netværket, udregne tabet, og køre bagudpropagering.\n",
    "        # Dette gøres for et antal epoker, og med en batchstørrelse.\n",
    "        \n",
    "        sidste_tab = 0\n",
    "        # Mini-batch gradientnedstigning.\n",
    "        i = 0\n",
    "        for epoke in range(epoker): \n",
    "            # For hver epoke\n",
    "\n",
    "            for batch in range(batchstørrelse): \n",
    "                # For hver batch\n",
    "\n",
    "                output = self.frem(inputs[i % len(inputs)])\n",
    "                # Inputtet sendes igennem netværket, og outputtet returneres.\n",
    "\n",
    "                pa_output = pa_tab(output, ønskede_outputs[i % len(ønskede_outputs)])\n",
    "                # Den partielle afledte af tabet for outputtet udregnes.\n",
    "\n",
    "                if debug: print(np.shape(pa_output))\n",
    "\n",
    "                self.__tilbage(pa_output)\n",
    "                # Partielle afledte sendes igennem netværket.\n",
    "\n",
    "                \n",
    "                \n",
    "                i += 1\n",
    "            \n",
    "            self.__update(skridt_længde)\n",
    "            # Efter at have kørt en batch igennem, opdateres vægtene og biaserne.\n",
    "            \n",
    "            if (epoke+1) % tab_print_interval == 0:\n",
    "                tab = self.__tab(output, ønskede_outputs[i % len(ønskede_outputs)])\n",
    "                print(f\"Epoke {epoke+1}/{epoker} | {np.round(100*(epoke+1)/epoker,2)}% | Tab: {tab} | Ændring: {tab - sidste_tab}\")\n",
    "                # Her printes tabet, så vi kan se, om netværket lærer noget.\n",
    "\n",
    "                sidste_tab = tab\n",
    "        \n",
    "        print(\"\\nTræning færdig\")\n",
    "        print(f\"Afsluttet med tab: {self.__tab(output, ønskede_outputs[i % len(ønskede_outputs)])}\")\n",
    "\n",
    "\n",
    "    def test(self,inputs : np.ndarray, ønskede_outputs : np.ndarray) -> None:\n",
    "\n",
    "        rigtige_svar = 0\n",
    "\n",
    "        for i in range(len(inputs)):\n",
    "\n",
    "            output = self.frem(inputs[i])\n",
    "            # Udregner outputtet for inputtet.\n",
    "\n",
    "            max_index = np.argmax(output)\n",
    "            output = np.zeros(output.shape)\n",
    "            output[max_index] = 1\n",
    "            # Finder det index, hvor outputtet er størst, og sætter det til 1, og resten til 0.\n",
    "\n",
    "            if np.array_equal(output, ønskede_outputs[i]): rigtige_svar += 1\n",
    "\n",
    "        præcision = np.round(rigtige_svar / len(inputs) * 100,2)\n",
    "        print(f\"Præcision: {præcision}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test af ANN\n",
    "Al den overstående kode er nok til at definere et ANN, så før jeg begynder at løse et stort datasæt, vil jeg lige teste om det fungerer først. Til dette bruger jeg den klassiske XOR test, der er et ikke-lineært separabelt problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoke 500/10000 | 5.0% | Tab: 0.1997841031068056 | Ændring: 0.1997841031068056\n",
      "Epoke 1000/10000 | 10.0% | Tab: 0.18705817409270933 | Ændring: -0.012725929014096277\n",
      "Epoke 1500/10000 | 15.0% | Tab: 0.16717442684623068 | Ændring: -0.01988374724647865\n",
      "Epoke 2000/10000 | 20.0% | Tab: 0.13769383398268165 | Ændring: -0.029480592863549038\n",
      "Epoke 2500/10000 | 25.0% | Tab: 0.09925065333078333 | Ændring: -0.03844318065189832\n",
      "Epoke 3000/10000 | 30.0% | Tab: 0.06321618379072792 | Ændring: -0.0360344695400554\n",
      "Epoke 3500/10000 | 35.0% | Tab: 0.03987572708172186 | Ændring: -0.023340456709006065\n",
      "Epoke 4000/10000 | 40.0% | Tab: 0.02686990074199055 | Ændring: -0.013005826339731307\n",
      "Epoke 4500/10000 | 45.0% | Tab: 0.019438607146084214 | Ændring: -0.007431293595906337\n",
      "Epoke 5000/10000 | 50.0% | Tab: 0.01488246355463083 | Ændring: -0.004556143591453385\n",
      "Epoke 5500/10000 | 55.0% | Tab: 0.011892340900798779 | Ændring: -0.0029901226538320506\n",
      "Epoke 6000/10000 | 60.0% | Tab: 0.009815966035912702 | Ændring: -0.0020763748648860766\n",
      "Epoke 6500/10000 | 65.0% | Tab: 0.008307111514715563 | Ændring: -0.0015088545211971397\n",
      "Epoke 7000/10000 | 70.0% | Tab: 0.00716983430160411 | Ændring: -0.0011372772131114529\n",
      "Epoke 7500/10000 | 75.0% | Tab: 0.0062867555551969195 | Ændring: -0.0008830787464071902\n",
      "Epoke 8000/10000 | 80.0% | Tab: 0.005584050872187707 | Ændring: -0.0007027046830092127\n",
      "Epoke 8500/10000 | 85.0% | Tab: 0.005013325426477004 | Ændring: -0.0005707254457107029\n",
      "Epoke 9000/10000 | 90.0% | Tab: 0.0045417138361920325 | Ændring: -0.00047161159028497133\n",
      "Epoke 9500/10000 | 95.0% | Tab: 0.004146210032233745 | Ændring: -0.00039550380395828727\n",
      "Epoke 10000/10000 | 100.0% | Tab: 0.003810282002776482 | Ændring: -0.0003359280294572631\n",
      "\n",
      "Træning færdig\n",
      "Afsluttet med tab: 0.003810282002776482\n"
     ]
    }
   ],
   "source": [
    "ann = Netværk([\n",
    "    FF(2, 3), \n",
    "    Sigmoid(), \n",
    "    FF(3, 1), \n",
    "    Sigmoid()\n",
    "    ])\n",
    "# Her opretter jeg et netværk, som består af et inputlag med 2 neuroner, et skjult lag med 3 neuroner,\n",
    "# et outputlag med 1 neuron.\n",
    "\n",
    "inputs = np.reshape([[0, 0], [0, 1], [1, 0], [1, 1]], (4, 2, 1))\n",
    "ønskede_outputs = np.reshape([[0], [1], [1], [0]], (4, 1, 1))\n",
    "# Her opretter jeg inputs og ønskede_outputs, som er XOR-gate. Dette er et klassisk problem, som\n",
    "\n",
    "ann.træn(inputs, ønskede_outputs, 0.1, 10000, 4, 500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her man man se, at vi har opnået et relativt lavt tab. Nu vil jeg prøve at køre den igennem for at se dets resultater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,0] : Expected 0 : Output 0.0\n",
      "[1,0] : Expected 1 : Output 1.0\n",
      "[0,1] : Expected 1 : Output 1.0\n",
      "[1,1] : Expected 0 : Output 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"[0,0] : Expected 0 : Output {np.round(ann.frem(np.array([[0], [0]]))[0,0])}\")\n",
    "print(f\"[1,0] : Expected 1 : Output {np.round(ann.frem(np.array([[1], [0]]))[0,0])}\")\n",
    "print(f\"[0,1] : Expected 1 : Output {np.round(ann.frem(np.array([[0], [1]]))[0,0])}\")\n",
    "print(f\"[1,1] : Expected 0 : Output {np.round(ann.frem(np.array([[1], [1]]))[0,0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den svarer rigtigt på opgaven."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importering af MNIST\n",
    "MNIST er et stort dataset af håndskrevne cifre fra 0 - 9. Idéen her at at få netværket til at kunne genkende tallene og sortere dem ind i de rigtige klasser. Men før dette kan gøres, skal vi importere denne data. I denne mappe har jeg to csv-filer med data. Den ene fil er til at træne netværker, mens den anden fil er til at teste dets præcision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Træningsdatet\n",
    "train_data = pd.read_csv('mnist_train.csv')\n",
    "\n",
    "train_labels = train_data.iloc[:, 0].to_numpy()\n",
    "train_images = train_data.iloc[:, 1:].to_numpy().reshape(-1, 784, 1) / 255\n",
    "\n",
    "# Testdataet\n",
    "test_data = pd.read_csv('mnist_test.csv')\n",
    "\n",
    "test_labels = test_data.iloc[:, 0].to_numpy()\n",
    "test_images = test_data.iloc[:, 1:].to_numpy().reshape(-1, 784, 1) / 255\n",
    "\n",
    "# Omdanner tallene i labels til vektorer med samme facon som outputtet fra netværket.\n",
    "def format_labels(labels):\n",
    "    formatted_labels = []\n",
    "    for label in labels:\n",
    "        formatted_label = np.zeros((10,1))\n",
    "        formatted_label[label] = 1\n",
    "        formatted_labels.append(formatted_label)\n",
    "    return np.array(formatted_labels)\n",
    "\n",
    "# Omdanner labels til vektorer\n",
    "train_labels = format_labels(train_labels)\n",
    "test_labels = format_labels(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Træning på MNIST\n",
    "Nu er det bare tid til at træne et netværk på mnist. Alle billederne i MNIST er 1x28x28, så får det første lag skal have 784 neuroner. Selve sættet er heldigvis allerede sat i tilfældig rækkefølge, så det behøver vi ikke at bekymre os om."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoke 16000/1280000 | 1.25% | Tab: 1.0465060230125232 | Ændring: 1.0465060230125232\n",
      "Epoke 32000/1280000 | 2.5% | Tab: 0.9665137359362692 | Ændring: -0.07999228707625394\n",
      "Epoke 48000/1280000 | 3.75% | Tab: 1.6698679161327308 | Ændring: 0.7033541801964616\n",
      "Epoke 64000/1280000 | 5.0% | Tab: 1.1654920346423918 | Ændring: -0.504375881490339\n",
      "Epoke 80000/1280000 | 6.25% | Tab: 1.2662435628018296 | Ændring: 0.10075152815943778\n",
      "Epoke 96000/1280000 | 7.5% | Tab: 1.280884800893523 | Ændring: 0.014641238091693376\n",
      "Epoke 112000/1280000 | 8.75% | Tab: 1.8165961251747014 | Ændring: 0.5357113242811784\n",
      "Epoke 128000/1280000 | 10.0% | Tab: 0.10763709104669086 | Ændring: -1.7089590341280105\n",
      "Epoke 144000/1280000 | 11.25% | Tab: 1.1970412417308736 | Ændring: 1.0894041506841827\n",
      "Epoke 160000/1280000 | 12.5% | Tab: 1.170272289430593 | Ændring: -0.026768952300280535\n",
      "Epoke 176000/1280000 | 13.75% | Tab: 1.6181014831018803 | Ændring: 0.44782919367128726\n",
      "Epoke 192000/1280000 | 15.0% | Tab: 1.7200296172230647 | Ændring: 0.1019281341211844\n",
      "Epoke 208000/1280000 | 16.25% | Tab: 1.8287721839856546 | Ændring: 0.10874256676258987\n",
      "Epoke 224000/1280000 | 17.5% | Tab: 1.2159246432957986 | Ændring: -0.6128475406898559\n",
      "Epoke 240000/1280000 | 18.75% | Tab: 1.9159161495794084 | Ændring: 0.6999915062836097\n",
      "Epoke 256000/1280000 | 20.0% | Tab: 0.016940228858410674 | Ændring: -1.8989759207209977\n",
      "Epoke 272000/1280000 | 21.25% | Tab: 0.23554309453407962 | Ændring: 0.21860286567566894\n",
      "Epoke 288000/1280000 | 22.5% | Tab: 1.300158341295752 | Ændring: 1.0646152467616723\n",
      "Epoke 304000/1280000 | 23.75% | Tab: 1.108728206563718 | Ændring: -0.19143013473203396\n",
      "Epoke 320000/1280000 | 25.0% | Tab: 1.1760248621497063 | Ændring: 0.06729665558598819\n",
      "Epoke 336000/1280000 | 26.25% | Tab: 1.89091879267361 | Ændring: 0.7148939305239037\n",
      "Epoke 352000/1280000 | 27.5% | Tab: 1.8926147937113285 | Ændring: 0.0016960010377184975\n",
      "Epoke 368000/1280000 | 28.75% | Tab: 1.9781230810492834 | Ændring: 0.0855082873379549\n",
      "Epoke 384000/1280000 | 30.0% | Tab: 1.3043109024849362 | Ændring: -0.6738121785643472\n",
      "Epoke 400000/1280000 | 31.25% | Tab: 1.6067641498715945 | Ændring: 0.30245324738665835\n",
      "Epoke 416000/1280000 | 32.5% | Tab: 1.759619425700801 | Ændring: 0.15285527582920655\n",
      "Epoke 432000/1280000 | 33.75% | Tab: 1.9514903509524073 | Ændring: 0.1918709252516062\n",
      "Epoke 448000/1280000 | 35.0% | Tab: 0.005977260276166369 | Ændring: -1.945513090676241\n",
      "Epoke 464000/1280000 | 36.25% | Tab: 0.007817464893894713 | Ændring: 0.001840204617728344\n",
      "Epoke 480000/1280000 | 37.5% | Tab: 1.0525051415560398 | Ændring: 1.0446876766621451\n",
      "Epoke 496000/1280000 | 38.75% | Tab: 1.94261491450886 | Ændring: 0.8901097729528202\n",
      "Epoke 512000/1280000 | 40.0% | Tab: 1.9274411040208008 | Ændring: -0.015173810488059214\n",
      "Epoke 528000/1280000 | 41.25% | Tab: 1.9436789190972696 | Ændring: 0.01623781507646882\n",
      "Epoke 544000/1280000 | 42.5% | Tab: 1.9239614451235305 | Ændring: -0.01971747397373913\n",
      "Epoke 560000/1280000 | 43.75% | Tab: 1.651432828598266 | Ændring: -0.2725286165252645\n",
      "Epoke 576000/1280000 | 45.0% | Tab: 1.987867118528649 | Ændring: 0.33643428993038293\n",
      "Epoke 592000/1280000 | 46.25% | Tab: 1.9657334255091843 | Ændring: -0.022133693019464662\n",
      "Epoke 608000/1280000 | 47.5% | Tab: 1.9023597102883725 | Ændring: -0.06337371522081181\n",
      "Epoke 624000/1280000 | 48.75% | Tab: 0.003490779527852491 | Ændring: -1.89886893076052\n",
      "Epoke 640000/1280000 | 50.0% | Tab: 1.8636809707413116 | Ændring: 1.860190191213459\n",
      "Epoke 656000/1280000 | 51.25% | Tab: 1.7932193546334112 | Ændring: -0.07046161610790036\n",
      "Epoke 672000/1280000 | 52.5% | Tab: 0.001649858152324691 | Ændring: -1.7915694964810864\n",
      "Epoke 688000/1280000 | 53.75% | Tab: 1.9901460815053669 | Ændring: 1.988496223353042\n",
      "Epoke 704000/1280000 | 55.0% | Tab: 1.9634518296444043 | Ændring: -0.026694251860962526\n",
      "Epoke 720000/1280000 | 56.25% | Tab: 1.379031744728098 | Ændring: -0.5844200849163064\n",
      "Epoke 736000/1280000 | 57.5% | Tab: 1.5711114516792462 | Ændring: 0.19207970695114818\n",
      "Epoke 752000/1280000 | 58.75% | Tab: 1.8510210746456432 | Ændring: 0.2799096229663971\n",
      "Epoke 768000/1280000 | 60.0% | Tab: 1.9227995889459097 | Ændring: 0.0717785143002665\n",
      "Epoke 784000/1280000 | 61.25% | Tab: 1.4145967323129496 | Ændring: -0.5082028566329602\n",
      "Epoke 800000/1280000 | 62.5% | Tab: 1.9655675676957833 | Ændring: 0.5509708353828338\n",
      "Epoke 816000/1280000 | 63.75% | Tab: 1.9260718919663682 | Ændring: -0.03949567572941515\n",
      "Epoke 832000/1280000 | 65.0% | Tab: 1.0459336123234766 | Ændring: -0.8801382796428916\n",
      "Epoke 848000/1280000 | 66.25% | Tab: 1.948108904662411 | Ændring: 0.9021752923389343\n",
      "Epoke 864000/1280000 | 67.5% | Tab: 1.9109664899595789 | Ændring: -0.03714241470283208\n",
      "Epoke 880000/1280000 | 68.75% | Tab: 1.9422045988600625 | Ændring: 0.03123810890048362\n",
      "Epoke 896000/1280000 | 70.0% | Tab: 1.96979064492622 | Ændring: 0.027586046066157444\n",
      "Epoke 912000/1280000 | 71.25% | Tab: 1.910095303886222 | Ændring: -0.05969534103999785\n",
      "Epoke 928000/1280000 | 72.5% | Tab: 1.9588878704180994 | Ændring: 0.04879256653187736\n",
      "Epoke 944000/1280000 | 73.75% | Tab: 1.9695091443464727 | Ændring: 0.01062127392837331\n",
      "Epoke 960000/1280000 | 75.0% | Tab: 1.3970118396603923 | Ændring: -0.5724973046860804\n",
      "Epoke 976000/1280000 | 76.25% | Tab: 1.95547606154423 | Ændring: 0.5584642218838376\n",
      "Epoke 992000/1280000 | 77.5% | Tab: 1.9488570659860882 | Ændring: -0.006618995558141716\n",
      "Epoke 1008000/1280000 | 78.75% | Tab: 1.23379039433754 | Ændring: -0.7150666716485483\n",
      "Epoke 1024000/1280000 | 80.0% | Tab: 1.9891294152809174 | Ændring: 0.7553390209433775\n",
      "Epoke 1040000/1280000 | 81.25% | Tab: 1.9522299357650899 | Ændring: -0.036899479515827505\n",
      "Epoke 1056000/1280000 | 82.5% | Tab: 1.957292591803585 | Ændring: 0.005062656038495206\n",
      "Epoke 1072000/1280000 | 83.75% | Tab: 1.954432938711812 | Ændring: -0.002859653091773051\n",
      "Epoke 1088000/1280000 | 85.0% | Tab: 1.9255095215878204 | Ændring: -0.0289234171239916\n",
      "Epoke 1104000/1280000 | 86.25% | Tab: 1.9238034556918655 | Ændring: -0.0017060658959549002\n",
      "Epoke 1120000/1280000 | 87.5% | Tab: 1.9804429211069843 | Ændring: 0.05663946541511877\n",
      "Epoke 1136000/1280000 | 88.75% | Tab: 1.9830924276108473 | Ændring: 0.002649506503862975\n",
      "Epoke 1152000/1280000 | 90.0% | Tab: 1.9840111479849105 | Ændring: 0.0009187203740632643\n",
      "Epoke 1168000/1280000 | 91.25% | Tab: 1.9572653776376636 | Ændring: -0.0267457703472469\n",
      "Epoke 1184000/1280000 | 92.5% | Tab: 1.9758307387941432 | Ændring: 0.018565361156479554\n",
      "Epoke 1200000/1280000 | 93.75% | Tab: 0.0012402715442672763 | Ændring: -1.974590467249876\n",
      "Epoke 1216000/1280000 | 95.0% | Tab: 1.725672825798913 | Ændring: 1.7244325542546457\n",
      "Epoke 1232000/1280000 | 96.25% | Tab: 1.9701229099862578 | Ændring: 0.24445008418734493\n",
      "Epoke 1248000/1280000 | 97.5% | Tab: 1.95098754978944 | Ændring: -0.01913536019681783\n",
      "Epoke 1264000/1280000 | 98.75% | Tab: 1.9337155690346126 | Ændring: -0.017271980754827387\n",
      "Epoke 1280000/1280000 | 100.0% | Tab: 1.9612165959540444 | Ændring: 0.027501026919431792\n",
      "\n",
      "Træning færdig\n",
      "Afsluttet med tab: 1.9612165959540444\n"
     ]
    }
   ],
   "source": [
    "mnistløser = Netværk([\n",
    "    FF(784, 16),\n",
    "    Sigmoid(),\n",
    "    FF(16, 16),\n",
    "    Sigmoid(),\n",
    "    FF(16, 10),\n",
    "    Sigmoid()\n",
    "])\n",
    "\n",
    "mnistløser.træn(train_images, train_labels, 0.01, 1280000, 10, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Præcision: 91.84%\n"
     ]
    }
   ],
   "source": [
    "mnistløser.test(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Netværk' object has no attribute 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Extract FF-layers from mnistløser\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m ff_layers \u001b[38;5;241m=\u001b[39m [layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmnistløser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, FF)]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Save FF-layers to a pickle file\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_layers.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Netværk' object has no attribute 'layers'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Extract FF-layers from mnistløser\n",
    "ff_layers = [layer for layer in mnistløser.lag if isinstance(layer, FF)]\n",
    "\n",
    "# Save FF-layers to a pickle file\n",
    "with open('ff_layers.pkl', 'wb') as f:\n",
    "    pickle.dump(ff_layers, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
